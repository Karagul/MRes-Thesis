{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "import itertools\n",
    "\n",
    "from lib.utes import run_experiment, plot_values, plot_greedy_policy, \\\n",
    "                        plot_action_values, random_policy, plot_state_value\n",
    "\n",
    "from lib.envs.grid import Grid, FeatureGrid, AltGrid\n",
    "from TD import RandomTD, GeneralQ, ExperienceQ, DynaQ, FeatureExperienceQ, FeatureDynaQ\n",
    "from FA.model import TabularModel, LinearModel\n",
    "\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "from lib.envs.windygridworld import WindyGridworldEnv\n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "from lib.envs.blackjack import BlackjackEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=1)\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAADYCAYAAADcQc6zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncDbBcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085941d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = Grid()\n",
    "maze.plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomTD(maze._layout.size, 4, maze.get_obs())\n",
    "run_experiment(maze, agent, int(1e5))\n",
    "v = agent.get_values()\n",
    "plot_values(v.reshape(maze._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q learning\n",
    "maze = Grid(noisy=False)\n",
    "\n",
    "agent = GeneralQ(maze._layout.size, 4, maze.get_obs(),\n",
    "                 SARSA=False, double=False)\n",
    "\n",
    "run_experiment(maze, agent, int(1e5))\n",
    "q = agent.q_values.reshape(maze._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarsa\n",
    "agent = GeneralQ(maze._layout.size, 4, maze.get_obs(),\n",
    "                 SARSA=True, double=False)\n",
    "\n",
    "run_experiment(maze, agent, int(1e5))\n",
    "q = agent.q_values.reshape(maze._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double Q learning\n",
    "maze = Grid(noisy=False)\n",
    "\n",
    "agent = GeneralQ(maze._layout.size, 4, maze.get_obs(),\n",
    "                 SARSA=False, double=True)\n",
    "\n",
    "run_experiment(maze, agent, int(1e5))\n",
    "q = agent.q_values.reshape(maze._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_q_learning = []\n",
    "mean_reward_double_q_learning = []\n",
    "mean_reward_sarsa = []\n",
    "\n",
    "for _ in range(20):\n",
    "  maze = Grid(noisy=True)\n",
    "  q_agent = GeneralQ(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=False, double=False, step_size=0.1)\n",
    "  dq_agent = GeneralQ(maze._layout.size, 4, maze.get_obs(),\n",
    "                      SARSA=False, double=True, step_size=0.1)\n",
    "  sarsa_agent = GeneralQ(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=False)\n",
    "\n",
    "  mean_reward_q_learning.append(run_experiment(maze, q_agent, int(1e5)))\n",
    "  mean_reward_double_q_learning.append(run_experiment(maze, dq_agent, int(1e5)))\n",
    "  mean_reward_sarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "\n",
    "plt.violinplot([mean_reward_sarsa,mean_reward_q_learning, mean_reward_double_q_learning])\n",
    "plt.xticks([1, 2,3], [\"Sarsa\",\"Q-learning\", \"Double Q-learning\"], rotation=60, size=12)\n",
    "plt.ylabel(\"average reward during learning\", size=12)\n",
    "ax = plt.gca()\n",
    "ax.set_axis_bgcolor('white')\n",
    "ax.grid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_grid = FeatureGrid()\n",
    "\n",
    "# Plot the features of each state\n",
    "shape = feat_grid._layout.shape\n",
    "f, axes = plt.subplots(shape[0], shape[1])\n",
    "for state_idx, ax in enumerate(axes.flatten()):\n",
    "  ax.imshow(np.reshape((feat_grid.int_to_features(state_idx)[:-1]),(9,9)), interpolation='nearest')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_greedy_policy(maze, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def random_policy(q):\n",
    "#  return np.random.randint(4)\n",
    "\n",
    "grid = Grid()\n",
    "agent = ExperienceQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   num_offline_updates=0, step_size=0.1)\n",
    "run_experiment(grid, agent, int(1e4))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "agent = ExperienceQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   num_offline_updates=30, step_size=0.1)\n",
    "run_experiment(grid, agent, int(1e4))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "model = TabularModel\n",
    "agent = DynaQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   model, num_offline_updates=30, step_size=0.1, eps =1.0)\n",
    "run_experiment(grid, agent, int(1e4))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "agent = ExperienceQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   num_offline_updates=0, step_size=0.1, eps=0.3)\n",
    "run_experiment(grid, agent, int(1e4))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "agent = ExperienceQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   num_offline_updates=30, step_size=0.1, eps=0.3)\n",
    "run_experiment(grid, agent, int(1e3))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here...\n",
    "grid = Grid()\n",
    "model = TabularModel\n",
    "agent = DynaQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   model, num_offline_updates=10, step_size=0.1, eps=1.0)\n",
    "run_experiment(grid, agent, int(2e3))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = FeatureGrid()\n",
    "\n",
    "agent = FeatureExperienceQ(\n",
    "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
    "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
    "  num_offline_updates=0, step_size=0.1, eps=1.0)\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "q = np.reshape(\n",
    "    np.array([agent.q(grid.int_to_features(i)) for i in range(grid.number_of_states)]),\n",
    "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = FeatureGrid()\n",
    "\n",
    "agent = FeatureExperienceQ(\n",
    "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
    "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
    "  num_offline_updates=10, step_size=0.1, eps=0.5)\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "q = np.reshape(\n",
    "    np.array([agent.q(grid.int_to_features(i)) for i in range(grid.number_of_states)]),\n",
    "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = FeatureGrid()\n",
    "model = LinearModel\n",
    "\n",
    "agent = FeatureDynaQ(\n",
    "  number_of_features=grid.number_of_features, \n",
    "  number_of_actions=4,\n",
    "  model=model,\n",
    "  number_of_states=grid._layout.size, \n",
    "  initial_state=grid.get_obs(),\n",
    "  num_offline_updates=10, \n",
    "  step_size=0.01,\n",
    "  eps = 0.5)\n",
    "\n",
    "run_experiment(grid, agent, int(1e5))\n",
    "q = np.reshape(\n",
    "    np.array([agent.q(grid.int_to_features(i)) for i in range(grid.number_of_states)]),\n",
    "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_steps = 2e4\n",
    "new_env_steps = pretrain_steps / 5\n",
    "\n",
    "\n",
    "# Train on first environment\n",
    "grid = Grid()\n",
    "agent = DynaQ(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "   model=TabularModel, num_offline_updates=30, step_size=0.1, eps=0.5)\n",
    "run_experiment(grid, agent, int(pretrain_steps))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_state_value(q)\n",
    "plot_greedy_policy(grid, q)\n",
    "\n",
    "# Change goal location\n",
    "alt_grid = AltGrid()\n",
    "run_experiment(alt_grid, agent, int(new_env_steps))\n",
    "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
    "plot_state_value(alt_q)\n",
    "plot_greedy_policy(alt_grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "#env = GridworldEnv()\n",
    "env = WindyGridworldEnv()\n",
    "#env = BlackjackEnv()\n",
    "\n",
    "#agent = GeneralQ(\n",
    "#  env.nS, env.action_space.n , env.reset(),\n",
    "#   model=TabularModel, num_offline_updates=30, step_size=0.1)\n",
    "\n",
    "agent = GeneralQ(env.nS, env.action_space.n, env.reset(),\n",
    "                 SARSA=False, double=True, step_size=0.1)\n",
    "\n",
    "\n",
    "agent = ExperienceQ(\n",
    "  env.nS, env.action_space.n, env.reset(),\n",
    "   num_offline_updates=10, step_size=0.1)\n",
    "\n",
    "\n",
    "agent = DynaQ(\n",
    "  env.nS, env.action_space.n, env.reset(),\n",
    "   model=TabularModel, num_offline_updates=21, step_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "mean_reward = 0\n",
    "discount_factor=0.95\n",
    "\n",
    "episode_rewards = np.zeros(num_episodes)\n",
    "episode_lengths = np.zeros(num_episodes)\n",
    "\n",
    "state = env.reset()\n",
    "#print(state)\n",
    "action = agent.behaviour_policy(agent._q[state])\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    #state = env.reset()\n",
    "      \n",
    "    for t in itertools.count():\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      #if done:\n",
    "      #      next_state = env.reset()\n",
    "\n",
    "      action = agent.step(reward, discount_factor, next_state)\n",
    "      #mean_reward += (reward - mean_reward)/(t + 1.)\n",
    "    \n",
    "      # Update statistics\n",
    "      episode_rewards[i_episode] += reward\n",
    "      episode_lengths[i_episode] = t\n",
    "        \n",
    "      if done:\n",
    "        agent._s = env.reset()\n",
    "        agent._last_action = agent.behaviour_policy(agent._q[state])\n",
    "        break\n",
    "    \n",
    "      state = next_state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards\n",
    "plt.plot(np.convolve(episode_rewards, np.ones((10,))/10, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "   \n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        # One step in the environment\n",
    "        # total_reward = 0.0\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            \n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q =  q_learning(env, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tabular import ExpTabAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze.get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "\n",
    "\n",
    "agent = ExpTabAgent(grid._layout.size, 4, grid.get_obs(),\n",
    "   SARSA=False, double=False, eps=0.2, step_size=0.1, num_offline_updates=30)\n",
    "\n",
    "run_experiment(grid, agent, int(1e4))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_steps = 2e4\n",
    "new_env_steps = pretrain_steps / 5\n",
    "\n",
    "#agent = ExpTabAgent(grid._layout.size, 4, grid.get_obs(),\n",
    "#   SARSA=False, double=False, eps=0.2, step_size=0.1, num_offline_updates=30)\n",
    "\n",
    "\n",
    "# Train on first environment\n",
    "grid = Grid()\n",
    "agent = ExpTabAgent(grid._layout.size, 4, grid.get_obs(),\n",
    "   SARSA=True, double=True, eps=0.1, model=TabularModel, step_size=0.1, num_offline_updates=10)\n",
    "\n",
    "run_experiment(grid, agent, int(pretrain_steps))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_state_value(q)\n",
    "plot_greedy_policy(grid, q)\n",
    "\n",
    "# Change goal location\n",
    "alt_grid = AltGrid()\n",
    "run_experiment(alt_grid, agent, int(new_env_steps))\n",
    "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
    "plot_state_value(alt_q)\n",
    "plot_greedy_policy(alt_grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the new stuff all works\n",
    "#Q learning SARSA=False, double=False\n",
    "#SARSA=True, double=False\n",
    "#SARSA=False, double=True\n",
    "\n",
    "#Q learning\n",
    "maze = Grid(noisy=False)\n",
    "\n",
    "\n",
    "agent = ExpTabAgent(grid._layout.size, 4, grid.get_obs(),\n",
    "   SARSA=False, double=False, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "\n",
    "run_experiment(maze, agent, int(1e5))\n",
    "q = agent.q_values.reshape(maze._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_q_learning = []\n",
    "mean_reward_double_q_learning = []\n",
    "mean_reward_sarsa = []\n",
    "mean_reward_dsarsa = []\n",
    "mean_reward_exp_q_learning = []\n",
    "mean_reward_exp_double_q_learning = []\n",
    "mean_reward_exp_sarsa = []\n",
    "mean_reward_exp_dsarsa = []\n",
    "\n",
    "#agent = ExpTabAgent(grid._layout.size, 4, grid.get_obs(),\n",
    "#   SARSA=False, double=False, model=None, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "\n",
    "for _ in range(10):\n",
    "  maze = Grid(noisy=True)\n",
    "  q_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=False, double=False, model=None, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  dq_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                      SARSA=False, double=True, model=None, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  sarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=False, model=None, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  dsarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=True, model=None, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  exp_q_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=False, double=False, model=None, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "  exp_dq_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                      SARSA=False, double=True, model=None, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "  exp_sarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=False, model=None, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "  exp_dsarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=True, model=None, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "\n",
    "  mean_reward_q_learning.append(run_experiment(maze, q_agent, int(1e5)))\n",
    "  mean_reward_double_q_learning.append(run_experiment(maze, dq_agent, int(1e5)))\n",
    "  mean_reward_sarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "  mean_reward_dsarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "  mean_reward_exp_q_learning.append(run_experiment(maze, q_agent, int(1e5)))\n",
    "  mean_reward_exp_double_q_learning.append(run_experiment(maze, dq_agent, int(1e5)))\n",
    "  mean_reward_exp_sarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "  mean_reward_exp_dsarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "\n",
    "plt.violinplot([mean_reward_sarsa,mean_reward_q_learning, mean_reward_double_q_learning, mean_reward_dsarsa,\n",
    "               mean_reward_exp_sarsa,mean_reward_exp_q_learning, mean_reward_exp_double_q_learning, \n",
    "              mean_reward_exp_dsarsa])\n",
    "plt.xticks([1, 2,3,4,5,6,7,8], [\"Sarsa\",\"Q-learning\", \"Double Q-learning\", \"DSarsa\",\n",
    "                     \"Exp_Sarsa\",\"Exp_Q-learning\", \"Exp_Double Q-learning\", \"Exp_DSarsa\"], rotation=60, size=12)\n",
    "plt.ylabel(\"average reward during learning\", size=12)\n",
    "ax = plt.gca()\n",
    "ax.set_axis_bgcolor('white')\n",
    "ax.grid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_q_learning = []\n",
    "mean_reward_double_q_learning = []\n",
    "mean_reward_sarsa = []\n",
    "mean_reward_dsarsa = []\n",
    "mean_reward_exp_q_learning = []\n",
    "mean_reward_exp_double_q_learning = []\n",
    "mean_reward_exp_sarsa = []\n",
    "mean_reward_exp_dsarsa = []\n",
    "\n",
    "#agent = ExpTabAgent(grid._layout.size, 4, grid.get_obs(),\n",
    "#   SARSA=False, double=False, model=None, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "\n",
    "for _ in range(10):\n",
    "  maze = Grid(noisy=True)\n",
    "  q_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=False, double=False, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  dq_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                      SARSA=False, double=True, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  sarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=False, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  dsarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=True, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=0)\n",
    "  exp_q_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=False, double=False, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "  exp_dq_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                      SARSA=False, double=True, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "  exp_sarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=False, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "  exp_dsarsa_agent = ExpTabAgent(maze._layout.size, 4, maze.get_obs(),\n",
    "                     SARSA=True, double=True, model=TabularModel, eps=0.1, step_size=0.1, num_offline_updates=10)\n",
    "\n",
    "  mean_reward_q_learning.append(run_experiment(maze, q_agent, int(1e5)))\n",
    "  mean_reward_double_q_learning.append(run_experiment(maze, dq_agent, int(1e5)))\n",
    "  mean_reward_sarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "  mean_reward_dsarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "  mean_reward_exp_q_learning.append(run_experiment(maze, q_agent, int(1e5)))\n",
    "  mean_reward_exp_double_q_learning.append(run_experiment(maze, dq_agent, int(1e5)))\n",
    "  mean_reward_exp_sarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "  mean_reward_exp_dsarsa.append(run_experiment(maze, sarsa_agent, int(1e5)))\n",
    "\n",
    "plt.violinplot([mean_reward_sarsa,mean_reward_q_learning, mean_reward_double_q_learning, mean_reward_dsarsa,\n",
    "               mean_reward_exp_sarsa,mean_reward_exp_q_learning, mean_reward_exp_double_q_learning, \n",
    "              mean_reward_exp_dsarsa])\n",
    "plt.xticks([1, 2,3,4,5,6,7,8], [\"Sarsa\",\"Q-learning\", \"Double Q-learning\", \"DSarsa\",\n",
    "                     \"Exp_Sarsa\",\"Exp_Q-learning\", \"Exp_Double Q-learning\", \"Exp_DSarsa\"], rotation=60, size=12)\n",
    "plt.ylabel(\"average reward during learning\", size=12)\n",
    "plt.title(\"Tabular model based\")\n",
    "ax = plt.gca()\n",
    "ax.set_axis_bgcolor('white')\n",
    "ax.grid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "#env = GridworldEnv()\n",
    "env = WindyGridworldEnv()\n",
    "#env = BlackjackEnv()\n",
    "\n",
    "agent = ExpTabAgent(env.nS, env.action_space.n, env.reset(),\n",
    "   SARSA=False, double=True, model=None, eps=0.1, step_size=0.1, num_offline_updates=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "mean_reward = 0\n",
    "discount_factor=0.95\n",
    "\n",
    "episode_rewards = np.zeros(num_episodes)\n",
    "episode_lengths = np.zeros(num_episodes)\n",
    "\n",
    "state = env.reset()\n",
    "action = agent.behaviour(state)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "      \n",
    "    for t in itertools.count():\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "      action = agent.step(reward, discount_factor, next_state)\n",
    "\n",
    "      episode_rewards[i_episode] += reward\n",
    "      episode_lengths[i_episode] = t\n",
    "        \n",
    "      if done:\n",
    "        agent.done(env.reset())\n",
    "        break\n",
    "    \n",
    "      state = next_state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards\n",
    "plt.plot(np.convolve(episode_rewards, np.ones((10,))/10, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Linear import FeatureExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lib/utes.py:102: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold('on')\n",
      "/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/matplotlib/__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/matplotlib/rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAHLCAYAAABvblmRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3W2QZFd93/Hff3oed2b2eVdaaVfPQghjIWQZsIMRWGAW\ngiBRXDaEBEThFy6b2CmXy8HY5RIuG0jiclGUiWMlxipcCRASx0hIZQGSUQzBaGtjSUhIsh5YsdJK\nWg3aJ+3szkP3Py+6e7rnoe+5PXN67pnb30/VlKb7nrn3zujs+fU5955zzd0FAAB6Y6DoEwAAoMwI\nWgAAeoigBQCghwhaAAB6iKAFAKCHCFoAAHqIoAWQJDP7nJkdNbOH2t7bbmZfN7PHG//dVuQ5AnkQ\ntABSdauk/Uve+6iku939ckl3N14DSTMWrACQKjO7SNJX3f3VjdePSXqzuz9nZnskfdPdryjwFIEg\nerQANpJz3P25xvfPSzqnyJMB8hgs+gQAlMv+/ft9ampq2fsHDx58UtKlbW993N1vXu1x3N3NjCE5\nJI+gBRDV1NSUDhy4b9n7AwOV4+5ua9z9C2a2p23o+Oga9wf0HEPHACJzSbUVvqK4TdIHG99/UNJX\nYu0Y6BWCFkB07rVlX90ysy9I+o6kK8zsGTP7sKRPSXqbmT0u6a2N10DSGDoG1sjMPinpBXf/dAHH\nvk/Sh9z94fU+dpbVBOvyffj7Omy6fs07B9YRPVpgDcxsl6QPSPqzgk7hjyT9fkHH7qCnQ8fAhkPQ\nAmtzk6Q73f1MQce/TdJbzOzcgo6/jHucoWOgLAhaIMDMBszstxvLAb5oZr9iZrON3uw7JN27pPwl\nZnaHmU2Z2Ukz+3pg/25ml7W9vtXM/qDt9aHG8b9vZsfM7C/MbFSS3P2spIOS3h7zd14bl3t12RfQ\nrwhaIOz3JL1L0lWSLlN9qPgld39R0o9LemxJ+c9LulP1xRR2S7o5wjm8X/UwvVTSKyT9btu2RyS9\nJsIxoqFHC7QQtECGRq/1NyS9392fd/cTku6Q9L1Gka2STi35sUslVSRV3P2su387wqn8ibsfdveX\nJP2hpPYbhU41ziMRLqm6whfQnwhaINv1kh5x90Nt7+1QK2iPSZpc8jPvl/QeSUfM7M/NbHuE8zjc\n9v3Tks5rez0p6XiEY0RDjxZoIWiBbDslvdh8YWaDkt4tqfnotgdVH8pd4O73uPv1kl6l+pDuTYFj\nTEva1PZ6pRub9rV9f4GkI22vr5T0QOAY68gJWqANQQtke1TST5vZxY1nn/6ppIvV6tHeKem6ZmEz\nu9HMLjczU72nuU3S/Y1tt5rZrSsc435J/9LMKma2v31/bX7VzPY2ese/I+lLjX2OSvoJSZk3XK0/\npvcATQQtkMHdvyHpy6r3GL+resDWJDUXiPi8pHea2Vjj9RtVvwv5lOoh/Cl3v6exbZ+kla7X/rqk\nG1Qf/n2/pL9eocx/l/Q1SU9JelJS867kG1R/VNyRFX6mEO70aIF2PI8W6IKZ/ZykzzYePN587xOS\njmatDGVmw6qH9VXuPtflMQ9J+qVG6C/d9l1JH3b3h5b9YEGuueYq/9a3bl/2/vj4RQfd/doCTgko\nFEswAt25Uq1hY0mSu38s9EPuPtv42ajc/fWx97l2Tg8WaEPQAt25Uq0bodABC1QALQQt0AV3/+UC\njnnReh9zbZrzaAFIBC2AHmDoGGghaAFE1bzrGEAdQYsNZfv2zb5v7+7MMrMnzgb3U5sND23muR9/\nwCxYxgbzzaLLU84qOY5XCe9nIMexBofHMrf/8IfPamrqWIcTYugYaCJosaHs27tbd972HzPLPPs3\njwf3c+rwiWCZPFPfRkaGg2WGto0Ey0jS8I5NwTIj20bDx9scPt7I9vCxdlxwdeb2N73pxg5b6NEC\n7QhaANFx1zHQQtACiIweLdCOoAUQlbvkPl/0aQDJIGgBRObiIQJAC0ELIDqu0QItBC2AyJygBdoQ\ntNhQZk+cDU7f+drdB4L7OfLSS8EytRzTe8ZHwlNpdm/ZEiwjSedu3Ross2NyMlhmYjQ8BWh0Z3h6\nj709e87u/OyZjK0MHQNNBC2AyOjRAu0IWgCREbRAO4IWQFT16T0MHQNNBC2A+HJc3wb6BUELIDJX\nrUqPFmgiaAHE5ZJq9GiBJoIWQHRO0AILCFoA0eV5xCDQLwhalM6mHItI7Ny8OVhmvhqeomI5Hvye\nZ1ELSdqzbVuwzJa94UUtbDB8Tnke/B58yHynzQwdA4sQtACicjlDx0AbghZAXM41WqAdQQsgOq7R\nAi0ELYDovErQAk0ELYC4GDoGFiFoAUTm3HUMtCFoAUTl9GiBRQhaANERtEALQYuNxUwWWGxhy9hY\ncDebhoeDZaZnZ4Nlnj92LFimMhBeHEKSdr1mT7jM6/YGy/h8eEH/mWNngmU2bT8nc/tAZajzORC0\nwAKCFkBcrAwFLELQAoiMlaGAdgQtgLhccp5HCywgaAFE5eIaLdCOoAUQlzN0DLQjaAFER9ACLQQt\ngOgIWqCFoAUQF9N7gEUIWmwsNVdtpppZZLBSCe5m08hIsEyeBSu+dOedwTJjExPBMpJ0zU++Mlhm\ny64rg2UGBkaDZU5vfSxYZmzswsBxOi/6QY8WaCFoAUTl3AwFLELQAojOa8yjBZoIWgBxueTz9GiB\nJoIWQHQMHQMtBC2AuLhGCyxC0AKIj2u0wAKCFkBU7pJX6dECTQQtgOgYOgZaCFoAcbnTowXaELTY\nWEyyimUWGR3uvGJR05axsVhnFHTm5ZdzlXv60WeDZS58+YlgmdGx84JlZl+eDpcZnsrc7j7feRs9\nWmABQQsgLpd8npuhgCaCFkBUPPgdWIygBRCXu7xKjxZoImgBxEePFlhA0AKIi3m0wCIELYC4CFpg\nEYIWQGRcowXaEbQAonLnrmOgHUGLjSXHHM3pmZngbsZHRoJl9p6zM1jmj3/914Nljp0+HSwjSRdc\nuidYZubYmWCZudOHgmXOHA0vojE0fiRze60213EbQ8dAC0ELIC53+RxDx0ATQQsgLhfXaIE2BC2A\n6LhGC7QQtACicklVJ2iBJoIWQFzuqtUYOgaaCFoAUbmkGj1aYAFBCyA6erRAC0ELICp3p0cLtCFo\nsbGYNDBcySwyM9d5IYWmPItIbN0yESxz0WsvCJa5bDK8OIYkjWwfC5apng3/bjMvTQfLzJ0ML+pR\nm6tmF8gI0yo9WmABQQsgKhdDx0A7ghZAXO5M7wHaELQAoqJHCyxG0AKIjpuhgBaCFkBU7s7NUEAb\nghZAdPRogRaCFkB0XKMFWghaAFG5u+YJWmABQYsNZXjLqPa+8xWZZQY3hxeI8Gp4aHPykm3hMheG\ny4yM7QmWkaRKJbxghXs4wObmfhQsU6udCZaZmPixzO2VwU0rvu9iwQqgHUELIDqGjoEWghZAVM6C\nFcAiBC2A6OjRAi0ELYCouEYLLEbQAoiKBSuAxQhaANExdAy0ELQAoqJHCyxG0AKIyiXNVQMPjQf6\nCEGLDeXB7z05dd6+f/Z00ecBSdKFK75LjxZYhKDFhuLuu4o+B2TjrmNgMYIWQFz0aIFFCFoAUdGj\nBRYjaAFERdACixG0AOLiMXnAIgQtgKhcUpXpPcACghZAVO6u2fn5ok8DSAZBCyAqVoYCFiNoAUTl\nkuYZOgYWELQA4nInaIE2BC2AqFjrGFiMoAUQldOjBRYhaAFE5e70aIE2BC2AqFzSHNN7gAUELYCo\narWazszOFn0aQDIGij4BAOXi7pqdm1v21S0z229mj5nZE2b20R6cKrAu6NECiKrmrplVBGs7M6tI\n+qykt0l6RtIBM7vN3b8f4RSBdUXQAojKazWdnZ5e625eJ+kJd39Kkszsi5LeI4mgxYZD0AKIqlar\nafrUqbXu5nxJh9tePyPp9WvdKVAEghZAVDPudz05O7tzhU1bzczbXn/c3W9ep9MCCkPQAojK3fdH\n2M2zkva1vd7beA/YcLjrGECKDki63MwuNrNhSe+VdFvB5wSsCj1aAMlx93kz+4ikuyRVJH3O3R8u\n+LSAVTF3D5cCAACrwtAxAAA9RNACANBDBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQtAAA9RNACANBD\nBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQtAAA9RNACANBDBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQt\nAAA9RNACANBDBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQtAAA9RNACANBDBC0AAD1E0GJFZvY5Mztq\nZg+1vbfdzL5uZo83/rutyHNEuVEHUbRYdZCgRSe3Stq/5L2PSrrb3S+XdHfjNdArt4o6iGLdqgh1\n0Nw9/qmhFMzsIklfdfdXN14/JunN7v6cme2R9E13v6LAU0TJUQdRtBh1kB4tunGOuz/X+P55SecU\neTLoS9RBFK3rOjjY2/PBetu/f79PTU0te//gwYNPSrq07a2Pu/vNqz2Ou7uZMRyCZfbv3+dTU2eX\nvX/w4BR1EOuiUzsoxW0L89ZBgrZkpqamdODAfcveHxioHHd3W+PuXzCzPW1DJkfXuD+U0NTUWd13\n4MZl71cGbqEOYl10agelKG1h13WQoePScUm1Fb6iuE3SBxvff1DSV2LtGOXhkmq+/CsS6iBy6NQO\nRmkLu66DBG0JudeWfXXLzL4g6TuSrjCzZ8zsw5I+JeltZva4pLc2XgPLxAha6iDWYqV2sNu2MFYd\n3JBDx2b2SUkvuPunV/nzhyT9krt/I0fZKyR9SfUx/d9x98+sUOY+SR9y94dXcz6xrSZYl+/D39dh\n0/Vr3nkfy1t3U6tTXYnUg6UOLrbWdq/DPh+W9Kvu/s0cZQ8pZ7vZKF9oHU6pHdxwPVoz2yXpA5L+\nbJ0O+VuS/tbdJ939M2Z2yMzeuqTMH0n6/XU6n4CeDh1jDbqsuwnVqe70eOi4L/Wq3XP3H8sTsnms\n0DYWWId7OnTctQ0XtJJuknSnu59Zp+NdKCn0iew2SW8xs3PX4XwyuccZOkZP3KT8dTeZOrUaBG10\nNyliu2dm6zGaWVgd7tQOFtUWJhm0ZjZgZr/dWPrqRTP7FTObbXyqe4eke5eUv8TM7jCzKTM7aWZf\n7/J455nZ/2oc6wdm9muN9++R9BZJf2JmLzfG6y+QdHvj9W9JkruflXRQ0tvX/tuvlSdTufqNmW02\nMzeznW3vvcrMXjCzLVpSd7PqbVp1qjv0aFen1+1eo8f578zsQUmnzWywvRdqZteY2T+Y2Skz+7KZ\nfcnM/mDJbq42swfN7ERj+2jjZ/9SS9rGYuvwyu0gQbvY70l6l6SrJF2m+pDJS+7+oqQfl/TYkvKf\nl3Sn6hOHd0u6Oe+BzGxA0u2SHpB0vupj7//WzN7u7j8r6e8kfcTdJxrj9T+UdEPj9X9o29Ujkl7T\n7S/aC+7VZV/oPXc/KemwpFe1vf2Hkv69u5/Q8robqrfJ1KluVX35F4LWo917n6R/Kmmru8833zSz\nYUn/W/UlB7dL+oKkf77Cz/+C6ksSXtw4z5skyd3/tVZuGwurwyu1g0W1hcndDNX49PYbkq5y9+cb\n790h6U2NIlslnVryY5dKqkiqND5FfbuLQ/6kpF3u3ryW8JSZ/RdJ75V0Vxf7OSVpTxfle8QlEawF\nekjSlZL+j5m9XtI1qtclaXndDdXbROpUd5webNfWsd37jLsfXuH9N6ieB5/x+rq8f9W4mWmlnz/S\nOL/bJV0dOF5BdTitdjDFHu31kh5x90Nt7+2Q9L3G98ckTS75mfdLeo+kI2b252a2vYvjXSjpPDM7\n3vyS9DF1v7TbpKTjXf5MT6QyXNKnHlKrR/sJSTe7+0zj9dK6G6q3ydSpbtVqy7+Qab3avZVCVpLO\nk/SsL178fqWyz7d9Py1pInC8wuowQ8fZdkp6sfmicdH+3ao3YJL0oKRXtP+Au9/j7ter3sC9Ro3h\njJwOS/qBu29t+5p093d2KN/ps/qVqg8/F4xrtAV7SNKrGte9zlV9eK9pUd3NUW8TqVPd4RrtqqxX\nu9fp/8Rzks43s/YVk/blOvPsfRdUh7lGG/KopJ82s4ut/py/P1X9ekDzk92dkq5rFjazG83s8kYF\nmZS0TdL9jW23mtmtgePdJ+lU4yaBMTOrmNmrzewnO5R/QdIl7W80bgj4CUld3YTVO2nc0t6nmj3a\nT6g+77p9/Gqh7mbV28b2xOpUd6iBXVvvdm+p76g+1vqRxk1S75H0ui73sahtLL4OM72no8Zk6C+r\n/inou6pXtJpaU2w+L+mdZjbWeP1G1e/GO6V6ZfyUu9/T2LZPgesWjYbwXapfa/iBpClJ/1XSlg4/\n8klJv9sYZv7Nxns3qP6opCNd/Ko94U6PtmDfV70nW3X3v16yrb3uZtVbKaE6tRr0aLuz3u3eCsef\nlXSjpA+rPtT7ryR9VdJM1s8tsbRtLKwOd2oHi2oLk38erZn9nKTPNh6y23zvE5KOZq2Q0riL7gHV\nby6Y6/E5flfSh939oWDhHrvmmqv8W9+6fdn74+MXHXT3aws4JbTJU3cb5ZKpU9266rW7/I57lz9U\n4IItt1AHc0qh3WvUwf/s7n+xhp8vpA53agelYtrC5O46XsGVag2fSJLc/WOhH2p8QruyVye15Fiv\nX4/j5OP0YBOWp+42yiVUp7pHD3bN1r3dM7PrVJ9CNKX6jVZXSfqb1eyrcS4F1uG02sGNErQb7lN9\nsdKpYOg/zZuhsCZFtHtXSPofksYlPSXp59secL4BpdMOJh+07v7LRZ/DxuJqm4cOrD+Xqum0cRtS\nEe2eu98i6Zb1Pm5vpNUOJh+06F5KQyboP/RokYKU2kGCtmSad9sBRSJoUaTU2sGugnb79s2+b9/u\nzDJzL88G9+Pz4T/A4nnTnQqFiyjPfiRZnolOA+F92UB4RwOVcJnBkbHM7U8//ax+9KNjHU4onaXH\nYtu6fdT37F26QM5iJ8NVUDmqYK7qlUfOKphrrt3ESLjMSCVcJs85DQVO6NChU5qaOrvinsoctDt2\nbMnRDoZnxdTmw3+kXFUnYjuoHLNQann+8eRY3NpCFUzS8JbRzO2Hnzmql146mXw72FXQ7tu3W39z\n5x9nlnnh208H9zN3/GywjA2G/yfkKTOQ43+mJA2MhP8Ug+ND4TKbhoNlRrZlh6gk7bgwex3u6677\n+Q5b0vokF9uevZP6izuXTx1pd88PwvuZmg6XyfG5KleZHNVUkjSa41/jdReGy1y4NVwmTxjvyf48\no2uv/asV3y/70PG+fbv1ta99JrPM899+KrifmR+Fn3iXq78xFP6fmbcdrM2Gw2lmKnze1VPhT7sj\ne8aDZfbuf0Xm9nfc8JsdtqTVDjJ0XEI8rQeFYoEKJCCldpCgLZ20Psmh/5S9R4uNIK12kKAtpXQ+\nyaE/EbQoXjrtIEFbMvW77dKpYOg/LubRoliptYMEbQmlVMHQh7hGiwSk1A4StKWT1ic59Cc6tChW\nWu1gV0E7e/ysfviVRzLL/P3fP5y5XZLmq+E/wKbh8DSZPHNtB3LMa5WkiZHwBMXtExPBMpt3BOZE\nSBo9L7yf0R2bMrdXq1nz9MrbzJ2alb55KLvM3+aY3nMsPMMs19SKPLVra/ZUwAU5Zn1pe44yL4Vn\nX+SacvS687O3z3WoZmW/GWr25IyevfsfM8scuPt7mdsl6eR0eI5ZLce81jxt5Z5t24JlJGnreHjK\nzfRMeI7w0y++GCxz+tHwft50OvsBRLPHsip7Ou0gPdrSSeuTHPpTmYMWG0Fa7SBBWzppVTD0n7L3\naLERpNUOErQl457WYtroTwQtipRaO0jQllGO6zpAz3DXMVKQUDtI0JaOq8YkRhSIebQoXlrtIEFb\nNlwgQwKogihUYu0gQVtCnlAFQ/9JrI1Dn0qpHSRoS8gTujaBPsQ1WiQgpXawq6CtzVb18rMnM8sc\nP306uJ+J0fAM/tEck7CrtfAY/Mxc9oTnprnB8J9iLMeiFhOXhyeGT16yPVhm2/afytw+ONhhYnnJ\nuxM1l2bms8vkWYxhc44HqM/lmB2Q50/9ql3hMpL0phzPmt0UfiSyJiP9buOBf4KVDgt6lLwK1gVW\nMxmq5HhGbI7FdM7mWBwiz0I6F161L1hGks67/tJgmTzP7r7g/4afS/6Dbz0ZPqE8D3xeSWKVkB5t\nybg8qSET9CeqIIqUWjtI0JaNp3VtAv2JKohCJdYOErQllNK1CfSfxEbt0KdSagcJ2jKilUORuBkK\nKUioEhK0ZeNSrZpOBUP/cUlUQRQqsXaQoC0duhMoHlUQxUqrHSRoS8YTuwkA/YdrtChaau0gQVtC\nKVUw9CeqIIqWUjvY3YIV7pqdz14tIM+dXuM5Fn7YsmlTsMzZ2dlgmRPT08EyecttHe+wSESbHcM5\nFr44ZzJYZmRkZ+Z2s87HSamCxTZgUmi+fGAtAUnSeI6FHwbCa6bobGDxDEnauzlcRpKuPjdcZmf4\nn4X+4flwmbkc661PBH7/TmsJeFqjdtFZxTQcWPFkW462opZjwZ08No+NBctMXBJeSEeStp/zhmCZ\n4eHwgjvV194RLHPmcPbiR5I0HKjwNtR5YZCU2kF6tGXDuB0SQBVEoRJrBwna0klrRRT0J6ogipVW\nO0jQlk1iNwGg/yTWmUA/SqwdJGhLxiV5Qg88Rh9yHvyOYqXWDhK0ZeNpDZmg/9CjReESawcJ2hJK\nqYKhP1EFUbSU2kGCtoRSqmDoP/RokYKU2kGCtmxo5ZAAqiAKlVg72FXQDphpZCh7pv/E6GhwP+M5\nyuTZz1Cl82TlppnAAhtNp8+eDZbJM8G8Oh1eRGP+9Eyuc1qtlD7J9UIlsCDFaI5aPTQQLjOeY8GK\nPPdbnMlXBXMtNPGKHeEy54XXQ9HhE+Eyq1b2BSvMZIPZFWg00E5K0s7N4ZVMdkyG/2fu2BwuU52e\nC5aRpB89951gmUqOf2AvHz4eLJNncaPhrdk5YBmNQUrtID3akvHEbgJA/0msM4E+lFo7SNCWkEda\n2g1YrYTaOPSplNpBgrZsXPJ5WjkUh+fRonCJtYMEbQmlNGSC/kQVRNFSagcJ2rJJ7NoE+k/Zn96D\nDSCxdpCgLaOErk2gPyXUxqFfJdQOErQl4y45F8hQMIIWRUqtHSRoSyilIRP0H6b3IAUptYMEbdm4\nJ/VJDv0poTYO/SixdrC7laGGKtq0azyzzNYXsrdL+VZNCa1AJUnjm8eCZXZftDNYRpKGtoX3NbIt\nx2pVW8Jl5nOs0nLmzHOZ22u1zvtI6ZNcbGZSJbCq06Zw1dFweFExTeRYGSpPmdEcx5Kkl8OLium5\nU+Ey28NVWSdzLE52JHCsuQ6XwEp/M5Rlr0gkSWMjI8HdTGwJt5XD28PtyXCO/+GVsRz/KCTNngpX\njBxVXkOT4d9/8rLtwTKbA2UqI50jLKV2kB5t2XhaE7XRnxJq49CPEmsHCdqScaU1URv9x8WD31Gs\n1NpBgrZs3OW0cigYPVoUKrF2kKAtI1o5FKns12ixMSRUCQnaskls/hj6D9N7ULjE2kGCtmwSq2Do\nTwQtCpVYO0jQlk5a1ybQf+jRonhptYMEbcm4pzV/DP2JKogipdYOdhW0NmAa3Jw9EXl4MLzLao75\nTWdnw7P3J8bCxxraEp44LUljeybC+xoPT9WuzswHy5w5+nKwzOk9j2Zur9XOdtyW0pBJdB5eK3w+\nxwfZ7OUG8u9nrhouczq8Pokk6Xjn/6UL8iy0kWcxiqOnw2WeOpa9vWNVL/vNUDka8Txt3MBsuPLU\ncpSpnglXsNnjZ4JlJOX6h+Hz4XOay7H6Si3HPx4bCJxQxuaU2kF6tGXjLu+0ZA+wDnjwOwqXWDtI\n0JaNK6lrE+hPpe7RIn2JtYMEbQmldG0C/YeboZCClNpBgrZk6sN26VQw9KeE2jj0odTaQYK2bNxV\nS2gxbfSf0j+9B+lLrB0kaEumPmxHK4diEbQoUmrtIEFbQil9kkN/ImhRtJTaQYK2ZNw9qU9y6D/c\nDIWipdYOdrdgxeCARraPZpYZHRoK7me+Gp6oPDMfXvhh6HR4UrSOhBeHkKS5U+F9VUbDf648E8xt\ncCBYZvz8qezjZPx9UqpgsZlJQ4FFG/I08mfC1Us5/jdpPFzdc4dOntkIear8dI4FMnKsJ6BTgYUv\nsubKljpoTRoIVMJqjjZuLkcbt+lEnqVVwmozOVZWkTTzo/DCFnn2NXcsvPpKngUlRs/JXkioljFX\nNqV2kB5tybjy/SMHesWdB7+jWKm1gwRt2bgndVs7+lOpe7RIX2LtIEFbMq60bgJA/+EaLYqWWjtI\n0JZQStcm0J8IWhQtpXaQoC0Zd8/15BCgV1iwAkVLrR0kaEsopU9y6E8ELYqWUjtI0JZQStcm0J8I\nWhQtpXaQoC2Z1IZM0H+4GQpFS60d7CpohyZGteefXJm9w4nh4H7mT4dn1FfGwqdWGQmXsYGcE75z\nlMu9r4DBTeG/0ZY9r8zcXhlaeeEQlzSfUAWLbXJYevNF2WXGw39enc2xYMVkjv2cmz2fXpK0c1O4\njCTlqM65AuxkYKEJScqxroou3569veM/US/3g9+HNm3SuVe/JrPM4KbwSibVHAs/DOVoT4cmR4Jl\nBvKsviJpPsdKLrPHw4tazJ4ML1gxOBr+G+185WWBfaz8u6fWDtKjLaGUhkzQf+jRIgUptYMEbcl4\nYhO10Z8IWhQptXaQoC2hlD7Jof/Qo0UKUmoHCdqScSmpmwDQnwhaFCm1dpCgLZnU7rZDH2LBChQs\ntXaQoC2hlIZM0H8YOkYKUmoHCdqSSe2THPoTQYsipdYOErQl45LmEnoOI/pP/fpY0WeBfpZaO9hV\n0D7wwD9O7dr1s0/36mTQlQtXfDexT3Kxff/Bqamr99xCHUxDhzpY7h7t/fc/MrVt27XUwTRsiHaw\nq6B19129OhHEkdrddrFRB9NX9mu01MH0pdYOMnRcNol9kkN/KnPQYgNIrB0kaEsmtU9y6E8ELYqU\nWjtI0JZMahUM/afsQ8dIX2rtIEFbNu5JPbUCfajkN0NhA0isHSRoS8YlVRO6rR39hx4tipZaO0jQ\nlown9km0L83yAAAOd0lEQVQO/YmgRZFSawcJ2pJxd83N53iqOdAjrnI/+B3pS60dJGhLxiXNJzRk\ngv5EjxZFSq0dJGjLxj2pCoY+xM1QKFpi7SBBWzKprfGJ/sPNUChaau0gQVsyntgnOfQnghZFSq0d\nJGhLxt2T+iSH/kOPFkVLrR0kaEvGpaTutkN/ImhRpNTaQYK2ZGrumk2ogqEPcTMUCpZaOzhQ9Akg\nLq/VdGZmZtlXt8xsv5k9ZmZPmNlHe3CqKKnmg9+XfnWLOojV6tQOFtUW0qMtmZq7Zubm1rQPM6tI\n+qykt0l6RtIBM7vN3b8f4RTRB9bao6UOYi1itINSvHpI0JaM12o6Oz291t28TtIT7v6UJJnZFyW9\nRxKNHIIi3QxFHcSqRWoHpUj1kKAtmVqtpulTp9a6m/MlHW57/Yyk1691p+gTca7RUgexapHaQSlS\nPSRoS2bG/a4nZ2d3rrBpq5m1N38fd/eb1+m00E+em7rLb76FOojCZLSDUgH1kKAtGXffH2E3z0ra\n1/Z6b+M9IIg6iKJFqoNSpHrIXcdYyQFJl5vZxWY2LOm9km4r+JzQX6iDSEGUekiPFsu4+7yZfUTS\nXZIqkj7n7g8XfFroI9RBpCBWPTR3ZpYDANArDB0DANBDBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQt\nAAA9RNACANBDBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQtAAA9RNACANBDBC0AAD1E0AIA0EMELQAA\nPUTQAgDQQwQtAAA9RNACANBDBC0AAD1E0AIA0EMELQAAPUTQAgDQQwQtAAA9RNACANBDBC0AAD1E\n0AIA0EMELYAkmdnnzOyomT3U9t52M/u6mT3e+O+2Is8RyIOgBZCqWyXtX/LeRyXd7e6XS7q78RpI\nmrl70ecAACsys4skfdXdX914/ZikN7v7c2a2R9I33f2KAk8RCKJHC2AjOcfdn2t8/7ykc4o8GSCP\nwaJPAEC57N+/36emppa9f/DgwSclXdr21sfd/ebVHsfd3cwYkkPyCFoAUU1NTenAgfuWvT8wUDnu\n7rbG3b9gZnvaho6PrnF/QM8xdAwgMpdUW+EritskfbDx/QclfSXWjoFeIWgBROdeW/bVLTP7gqTv\nSLrCzJ4xsw9L+pSkt5nZ45Le2ngNJI2hYyCDmX1S0gvu/ulI+7tV0jPu/rsx9pfzmPdJ+pC7P7xe\nx1xNsC7fh7+vw6br17xzYB3RowU6MLNdkj4g6c+KPpc1+iNJv79+h+vp0DGw4RC0QGc3SbrT3c8U\nfSJrdJukt5jZuetxMPc4Q8dAWRC06GtmNmBmv91Y6u9FM/sVM5tt9GbfIeneJeUvMbM7zGzKzE6a\n2dcD+3+tmf0/MztlZl+SNLpk+5Vm9k0zO25mD5vZuxvvf8jMbm8r97iZfbnt9WEzu7rx/SEz+00z\ne9DMTpjZl8xs4TjuflbSQUlvX/UfqitO0AJtCFr0u9+T9C5JV0m6TPWh4pfc/UVJPy7psSXlPy/p\nTtUXStgt6eZOOzazYUl/LekvJW2X9GVJ/6Jt+5Ck2yV9rbGvfyPpv5nZFaoH/M80PgicJ2lY0k81\nfu4SSROSHmw73C+ovlzhxY3f5aYlp/OIpNcE/hYRMXQMNBG06FuNXutvSHq/uz/v7ick3SHpe40i\nWyWdWvJjl0qqSKq4+1l3/3bGId4gaUjSp919zt3/p6QDS7ZPSPqUu8+6+z2Svirpfe7+VOPYV0t6\nk6S7JB0xs1dKuk7S3/nibuJn3P2Iu7+kenhfveRcTjV+n3Xgcp9f9gX0K4IW/ex6SY+4+6G293ao\nFbTHJE0u+Zn3S3qP6qH352a2PWP/50l61hcvKP70ku2HlwTm05LOb3x/r6Q3qx6090r6puohe52W\nDGmrvhxh07TqAd5uUtLxjHONiqFjoIWgRT/bKenF5gszG5T0bknNx7I9KOkV7T/g7ve4+/WSXqX6\nUOxNGft/TtL5Zta+GtIFbd8fkbTPzAaWbH+28X0zaH+m8f296hy0IVdKeqDLn1klrtEC7Qha9LNH\nJf20mV3ceK7pn6p+jbPZo71T9VCTJJnZjWZ2eSM4JyVtk3R/Y9utjTmy7b4jaV7Sr5nZkJndKOl1\nbdu/q3rv87ca298s6QZJX2xsv1fSWySNufszkv5O9euwOyT9Q95fsnFj1E9IyrxxKy6u0QJNBC36\nlrt/Q/UblB5QPfS+p3oiNBd2+Lykd5rZWOP1G1UPv1Oqh/CnGtdVJWmfpEXXa919VtKNqvd6X5L0\ni5L+asn2G1S/u3lK0n+S9AF3f7Sx/R8lvax6wMrdT0p6StK33b3axa96g+qPkzvSxc+smjs9WqAd\nz6MFGszs5yR9tvFQ8eZ7n5B0NGtlqMbdxQ9Iusrd53p/pt0xs+9K+rC7PxQsHME111zl3/rW7cve\nHx+/6KC7X7se5wCkhCUYgZYr1Ro2liS5+8dCP9TomV7Zq5NaK3d//TofkR4s0IagBVquVOtGKKwJ\nQQs0EbRAg7v/ctHnUA7OvFmgDUELIDqGjoEWghZAVM27jgHUEbTYUHbs2Or79mU/hGbudPhhO7WZ\n8OyY6mx4+HO2Gt5PZSDfLLrRyZFgmaGJsWCZgYHRYJnFa2h0KpPdPBw6dEhTU1MddtTN7COg3Aha\nbCj79p2rb3zjlswyz30n/Hzz00+FVyM8fiRc5rnj4TKbx8LhKEmXvenyYJnzfiZ8c/PExBXBMgMD\n4VAfGdmZuf3aazvN1KFHC7QjaAFE1916GkC5EbQAIqNHC7QjaAH0AD1aoImgBRAVdx0DixG0AKJj\nwQqghaAFEJlzMxTQhqDFhjJ3+oxeuO+RzDLf/3r2dkk6euJEsMyho0eDZQ4+8USwzAW7dwfLSNIv\nDg8Hy2zaMxksM3f+yWAZyzG3d8fuN2Zuz+61MnQMNBG0ACKjRwu0I2gBREbQAu0IWgBRufNQAaAd\nQQsgPveizwBIBkELIDqvEbRAE0ELIC53eZWhY6CJoAUQHT1aoIWgBRCdc40WWEDQYmNxV3Ume3m/\nwRyLMeycDC/8kGc/I0NDwTK7t2wJlpGk3ZftCpYZP29zsMzoRHg/tdpssEylMp653ayy8gaXRI8W\nWEDQAojK5QwdA20IWgBxOddogXYELYDouEYLtBC0AOKjRwssIGgBxOVSrUrQAk0ELYDInB4t0Iag\nBRCVczMUsAhBCyA6ghZoIWixoXhNmp/OXrCiWguvszs+OhosMzk2FiyzI8fCF1s2bQqWkaTRc7IX\niJAkWbhItTodLFOrzay5TNaj8AhaoIWgBRAXK0MBixC0ACJjZSigHUELIC5uhgIWIWgBROUiaIF2\nBC2AuHjwO7AIQQsgOnq0QAtBCyA6ghZoIWgBxMX0HmARghYbiplUGR7ILDMyNBTcz6bh4WCZscnw\nohZbqxPBMpWJ8PlIklWyfy9Jmj0ZXmiiOpO9oIck1ebDQTg78VLmdvdq520ELbCAoAUQlTvzaIF2\nBC2A6DzHMphAvyBoAURHjxZoIWgBxOWS57gGDPQLghZAXFyjBRYhaAHExzVaYAFBCyAqd8mr9GiB\nJoIWQHQMHQMtBC2AuNzp0QJtCFpsLCbZYPYKSu7hRr6a4xpinpWaBifD/4Qqm/KtDJXH7ImzwTI+\nH/7danOdV3Vqmt75VPY+ap1XqaJHC7QQtADichasANoRtADiY+gYWEDQAojK3VXLMXwN9AuCFkB8\nXKMFFhC0AOJiHi2wCEELIC6CFliEoAUQmcurXKMFmghaAFG5M48WaEfQYkPxmlQ9O59Z5sT0dHA/\necrMzGcfR5ImRkeDZUYmR4JlpHzhNHvsTLDM/Om5YJnabHjBitHd45nbq3Odj8PQMdBC0AKIyxk6\nBtoRtADicsnnCFqgiaAFEB3XaIEWghZAVC6pmuPBDkC/IGgBxOWuGg8VABYQtACickk1erTAAoIW\nQHT0aIEWghZAVO5OjxZoQ9BiQ7GKaXBiOLNMZWAguJ/p2dlgmeOnTwfLzGQs2tA0NjMTLCNJ46fC\n5fL8brM5FtoYrFSCZYILX2TcWUzQAi0ELYCoXFK1Gl55CugXBC2AuNyZ3gO0IWgBROXiZiigHUEL\nIDqu0QItBC2AqNxdVXq0wAKCFkB09GiBFoIWQHRcowVaCFoAUTF0DCxG0GJDGZ7YpL1vvCazzOB4\n9oIWkjT70pnwwQYsXGQwvIDEwEh4cQhJqoyE/zlaJXy8PA9dz/M32vWKq7L3Mbpp5eOLoWOgHUEL\nILp5FqwAFhC0AKJyFqwAFiFoAUTHzVBAC0ELICqXuBkKaEPQAoiKu46BxQhaANExdAy0ELQAoqJH\nCyxG0AKIjqAFWghabCj33//o1Pbtb3i66POAJOnCld50d80xjxZYQNBiQ3H3XUWfA7Jx1zGwGEEL\nIC6u0QKLELQAoqJHCyxG0AKIiqAFFiNoAcTlrnmCFlhA0AKIyiVVuesYWEDQAojK6dECixC0AKKj\nRwu0ELQAoqq5a3Z+vujTAJJB0AKIy13z9GiBBQQtgKhcYglGoA1BCyAqp0cLLELQAoiKhwoAixG0\nAKJySXPcDAUsIGgBRMVdx8BiA0WfAIBycXfNzc8v++qWme03s8fM7Akz+2gPThVYF/RoAURVq9V0\n+uzZNe3DzCqSPivpbZKekXTAzG5z9+9HOEVgXRG0AKLyWk1np6fXupvXSXrC3Z+SJDP7oqT3SCJo\nseEQtACiqtVqmj51aq27OV/S4bbXz0h6/Vp3ChSBoAUQ1Yz7XU/Ozu5cYdNWM/O21x9395vX6bSA\nwhC0AKJy9/0RdvOspH1tr/c23gM2HO46BpCiA5IuN7OLzWxY0nsl3VbwOQGrQo8WQHLcfd7MPiLp\nLkkVSZ9z94cLPi1gVczdw6UAAMCqMHQMAEAPEbQAAPQQQQsAQA8RtAAA9BBBCwBADxG0AAD0EEEL\nAEAPEbQAAPTQ/wdi0G3zRyaSqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fe26e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAADYCAYAAADcQc6zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEClJREFUeJzt3W1sU9cdBvDnJCG8JGyEJYFmlKgZqOVtTOoYUio+EET5\n0C5FmlShyoIxFE3qqmkSaVc+ZEFo0jK1K1+AVhrSYDBtFBFKKakq2ihoaUtBtCIaEwwVRKoqvARo\nSkJM3s4+XJtegh372ufY557z/CQL8MuTa9/7j6997QchpQQR2aEg3wtAROpwoIkswoEmsggHmsgi\nHGgii3CgiSzCgQ4BIcRWIcT+PP78fiFETZLLfimE6Mz1MlFiRfleAPIGxvfPaQDuARiN/fvXuV+i\nB0kpS/O9DJQePkMbQEpZGj8B6Abwc995/8jXcgkh+As/ZDjQ4VEshPi7EOKOEOKcEOKn8QuEEFVC\niENCiBtCiMtCiN8mCxFC/EAIcVQI8a0Q4rQQ4o/+XWYhhBRC/EYIcRHARd9583y3fzd2+1MAfqTv\nLlNQHOjwqAfwLwAzALwLYAcACCEKABwFcBbADwGsAvA7IcSaJDk7AQwAmA1gQ+w03loAywEsTHL7\nKIBHAPwqdiJDcKDDo1NK2SalHAWwD8DS2PnLAFRIKbdJKYeklJcA/BXAuvEBQohCAL8A0CylvCul\n/C+AvQl+1p+klLeklINJbv8HKeWAlPI/SW5PecLXSOFx1ff3uwCmxF7jVgOoEkJ847u8EMC/E2RU\nwFvnX/nO+yrB9RKdl+z2V1IsN+UQBzr8vgJwWUo5P43r3gAwAmAOgP/Fzns0wfWSfQUvfvtHAZyP\nnTc3/UUl3bjLHX6nANwRQvxeCDFVCFEohFgshFg2/oqx3fVWAFuFENOEEE8AWJ/uD0pw+4VI/Bqc\n8oQDHXKxIXsWwE8AXAbQC2A3gO8nuclLscuuwnst/k94x73T9RKA0tjt9wD4WybLTXoIFhy4TQjx\nZwCzpZR8prUAn6EdI4R4QgjxY+H5GYBNAA7ne7lIDb4p5p7p8HazqwBcA/AXAEfyukSkDHe5iSzC\nXW4ii3CgiSwS6DV0oRBS1YvuodifxYbm6ch0LU9Hpul5ujJHAIxKKVJdL9B8FsH7RL8K3bE/Tc3T\nkelano5M0/N0ZV5NfRUA3OUmsgoHmsgiHGgii3CgiSyiZaDH8N07fSbmkZlM327CsB1qGehhAHcM\nziMzmb7dhGE7dHqXewje8b1MSXjVIbpku3w68nTfZ8qO0wMt8V0FRya3vYVgXyTO5Gdkunw68nJx\nnyk7znzbagDAtwnOH4XXCBD0QwD9scxJAHrGXVYEr3wrn8unOg9Qf59JPeUD7f/tPQRv5af8vFoO\n8kpiJ78ReM9YZRnm3Y39qeK/ldCxfCrz4pkq77OfqduNrjxdlO9yDwK4De9O34T3zqBJeX7DAGYC\nmJzBbQvgPSOpXJ7xslk+HXk677Pp243O7VAl5QM9A94GI+Gt/ELD8vymIrthKQDwPUXLkki2y6cj\nT9d9Nn270bkdqqTlNXQZvAdA1S6J6jwyk+nbTRi2Q23vcqu+0yY/iKSO6duN6duh04etiGzDgSay\nCAeayCKBWj+FEKwIJcqDYgD30qgg4jM0kUUCHbYqhvouJ1W94EIIpXk6Ml3L05Fpep4/U+V/y8lO\nMSIHcaCJLMKBJrIIB5rIIqHoFOvr68PZs2eNzSM1TF/PYdhuQtEpdvHiRWzfvt3YPFLD9PUchu2G\nu9xZGBsbw9GjR43NG6+rqwuXLl0yPpMyx4HO0NjYGDZu3IjOzk4j8xKJRqNYu3at0gHUkUmZc6ZT\nTLW33noL+/btw8KFC3Hs2LEHLps/fz4OHz6c17z9+/ejpaXlofN7enqwbt06nDp1KlCerkxSy/hO\nsU8//RSFhV4/xJkzZ7B48WJMnpx574aqvPXr1+Ptt9/Gxo0bsWHDhoyXR1deJBJBJBJ54Lzu7m7U\n19dn/DpQR2acqetZV54ugb6cMVkImeqjn98AiMKragGASiSua0n3o5+vvvoqOjo6MDAwgMLCQnzw\nwQeYNWvWQ9dL9yN86ealkzkwMIA333wTjY2NE/7MfOWNd/z4cZSUlKC2tlZJnsrMfK1nnduN6o9+\npvPlDEgp0z4VA3JuGqfpgCwCZNUE10HslI7GxkY5b948+fXXXye9juq8oJnpcC0vaGY+1rPO7Sad\nWUn3VOwtY8oZVf4Mff8XBSbe1Q765YzR0dH7uzyJBH12SZWXSWYqruVlkpnr9axzu8nHM3RoOsVS\nPYj5ziM1TF/Ppm83PGxFZBEONJFFONBEFmGnGFEIsFOMyEHsFMthpurDGfHH0NQ8f6ap65mdYkRk\nLA40kUU40EQW4UATWYSdYmQtF7cbpzrFDh48iMcffxzl5eVYvnw56urqMDY2pmAJyUTsFLNYb28v\nIpEIiouLsWXLFixZsgSA2sMVRPnmzEBfunQJQ0NDmDt3LlauXIlFixahvb09q2/PqC71kwDuKkt7\n2BCAEcMzWTqYHWcGesGCBSgvL0dbWxtWrVqF9957D7t37844T3WpnwRwCw9WOKkmAdyA2gFUncnS\nwew40yk2ffp0fPzxx3jllVdw8uRJXLt2DQ0NDaisrER9fX3gPNWlfv0ABuA9Xj3jLisCUBFw+QYA\nfJvg/FEAvcjsE3+qM3WWDrJTLA1h7hQbHh5GUVERtmzZgo6ODly+fBnXr19HU1MTtm3blvA2E2X2\n9/fj2WefDVTqN9FHAsfgPdOVAChNKy34RzVHYj9jJoBEm2ImH/1MNzPd7SxeOrhz50489dRTD13O\nTrGJKd/lngFvxUp4zyrZ9ju0tLRgxYoViEajaGtrS/ogpnLu3DksWrQI06ZNQ1lZGe7c8d6HT1Zu\nl0ppaSmOHTuGGzduZHT78QrgPV4633MfRvLBMyXzwoUL2LVrV8JhDkLVdqMrTxdnOsWuXr2KF198\nESdPnsTNmzdRUVGBhoYGNDc3Z5wZFL+ckT12ik1MW9G+aZ1is2fPRmtrq6KlobBgpxgRhRYHmsgi\nHGgii3CgiSzCkkCiEGBJIJGDWBKYw0weh84eSwInxmdoIotwoIkswoEmsggHmsgiTg60jrK3MBTI\nuWBgYACbN29GdXU1Jk2ahPLycjz33HPo7u5OfeMUwrCOnRxoHWVvYSiQs52UEs888wzeeOMN1NTU\noLGxEdXV1bh27ZqSgQ7DOtb2basheL8tVP2Arq4ulJaWoqamRlGi+VQ/hqrzdGRms57b29tx4sQJ\nLFiwAB9++CG++OIL9PT0YM+ePYhGo4qW0GzanqHZNZU91Y+h7Z1iZ86cAQA8/fTTKCwsxL179xCN\nRtHb2+tMXbOSX6xh6poylerH0MVOsfgHOuJ/Hjp0CAcOHMCBAwfQ3NyMrVu3BlzC8FEy0CWxk1+8\na6osg7xIJIJIJPLAefGuqWxfw+goe1ORqfoxVJ2nI1P1en7yyScBAB999BE++eQTrFy5Eu3t7Th7\n9ixGRrLbh3C2JDBuEN7+fLK7HPQjgcePH0dJSUnSDjCdZW+qC+nS/Uhguo9hvvKCZOZiPUspUVdX\nh46ODjz22GMQQuD27du4ffs2Nm/ejNdffz1Qnl9YSgK1DXQq+fyM78svv4x33nkHJ06cQFVVVc4y\n+Vnu7KVaJ/39/WhqasLBgwfR09ODgoICrFmzBs3NzVi2bFngPL+g2w0HOgs6y95UF9KZOoA2DPR4\nrpUEOnkcGtBT9mZ6gZyLWBJIRKHFgSayCAeayCLsFCMKAXaKETmInWI5zORhq+yxU2xifIYmsggH\nmsgiHGgii3CgiSzi5ECHoVNsDF4bCFEQTg50GDrFhgHcUZYWDn19ffj888+dydNB20APQW3VTVdX\nl1P1QzqoXicqM/v6+rBmzRrU1tbi/ffftz5PF3aKOcTkTrGGhgbU1tairq4OTU1NWbd0mp6nCzvF\nLBWGTjG/vXv34ty5c9ixYwdaW1sxZcqULNLMz9OFnWKGdIr53fP9fQjAJAApP8Q7Thg6xfymTp16\n/+8qhsX0PF209XIPA5iJ5F1TQV24cAG7du1K2jWVriNHjtzvhtq0adOE3VD5yhwEEIW3O3sTQCUA\nFV+rV71OdGVS5rS9hp4KtSt59erVWQ8zALS0tGDFihWIRqNoa2vLeph1ZM6A99hJABVQM8yA+nWi\nK5My5+Rhq9deew3nz5+fsOgt35llAB6Bxl0ospKzJYH5yOS3rbLn8ret+H1oIsdwoIkswoEmsgg7\nxYhCgJ1iRA5ip1gOM13L05Fpep4/k51iRJQVDjSRRTjQRBbhQBNZhANNZBEONBnD9A4wpzvFiIIw\nvQMsLJ1i2r5tNQTvt0WyA91Bj0N3dXWhtLQUNTU1CS/P5Hii6kzX8lRmPv/885gzZw7Onz+P69ev\no7W1FXPnPnwk1/Q8f6ZV37YKQ0mg6kzX8lRm7t27Fy+88AIqKyvR2dmZdFhsydPFmZJA1Zmu5enK\njDO9AywsnWKQUqZ9Kgbk3DRPVYCcBMhZSS5H7JSuK1euyKVLl8rOzs6ElwfN05HpWp7qzNOnT8sN\nGzZMeB3T8/yZ6c5KOqdibxlTzqhzJYE6M13L05VJmdM20FNTXyWQ1atXK05Un+lanq5Myhw7xXKY\n6VqejkzT8/yZVr3LTUS5x4EmsggHmsgi7BQjCgF2ihE5iJ1iOcx0LU9Hpul5/kx2ihFRVjjQRBbh\nQBNZhANNZBEONJFFONBkDNM7wNgpRpQm0zvA2CkGdorZnqcyk51iE8v7t63YKWZ/nspM0zvA2CkG\ndorZlKcrM870DjB2irFTzPo81ZnsFGOnmFGZruXpyqTMsVOMecZlUubYKZbDTNfydGSanufPtOpd\nbiLKPQ40kUU40EQW4UATWYQlgUQhwJJAIgexJDCHma7l6cg0Pc+fyZJAIsoKB5rIIhxoIotwoIks\nwoFWxPT+KtPzdGSanqcDB1oB0/urTM/TkWl6ni7sFGMflra8MCyjbZ1i2hpLZsUaS6oUNZZ89tln\ncsmSJfLLL7+csCUiCFWZd+/evd9mMTg4mPR6ruWFYRl13ufQNpa42CnmZ3p/lel5OjJNz9NFyUCX\nxE5+I/BaP8syyItEIohEIg+c193djfr6emzfvj2jZdSRSWQadoqxD4sswk4xIouwUyyHma7l6cg0\nPc+fyU4xIsoKB5rIIhxoIotwoIkswk4xohBgpxiRg/LeKabqrX3VeToyXcvTkWl6nq5MdooROYgD\nTWQRDjSRRTjQRBbhQBNZhANtqDF4NU6u5OnIND1PB20DPQSv5MDUPB2ZqvLGAFyHd6hi0IE8HZmm\n5+mibaAlvMYSVQOjOk9Hpqq8W/CKIaYA6HMgT0em6Xm6GNkppjpPR6aOZYybCa/xpR9ABYDUVY/h\nztORaXqeLkZ2iqnO05GpYxnj/LtNKjYc0/N0ZJqep4u2XW7VnWKq83Rk6lhGoiBC0ymmOk9Hpo5l\nJAqCh62ILKLtGZqyMxlqd91Nz9ORaXqeDnyGJrIIB5rIIhxoIotwoIksErQk8AaAK/oWh4iSqJZS\nVqS6UqCBJiKzcZebyCIcaCKLcKCJLMKBJrIIB5rIIhxoIotwoIkswoEmsggHmsgi/weUL1RA7XcQ\nXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fe26b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####test on this stuff\n",
    "\n",
    "grid = FeatureGrid()\n",
    "\n",
    "#agent = ExpTabAgent(env.nS, env.action_space.n, env.reset(),\n",
    "#   SARSA=False, double=True, model=None, eps=0.1, step_size=0.1, num_offline_updates=20)\n",
    "\n",
    "\n",
    "#update changed target policy bit but sarsa doesnt seem to work model=LinearModel,model=LinearModel, double =True,\n",
    "\n",
    "agent = FeatureExp(\n",
    "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
    "  SARSA=True, double =True,model=LinearModel,\n",
    "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
    "  num_offline_updates=5, step_size=0.1, eps=0.1)\n",
    "run_experiment(grid, agent, int(3e5))\n",
    "\n",
    "q = np.reshape(\n",
    "    np.array([agent.q(grid.int_to_features(i)) for i in range(grid.number_of_states)]),\n",
    "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
    "plot_action_values(q)\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, number_of_steps):\n",
    "    mean_reward = 0.\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "    for i in range(number_of_steps):\n",
    "      reward, discount, next_state = env.step(action)\n",
    "\n",
    "      action = agent.step(reward, discount, next_state)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = FeatureGrid()\n",
    "grid.get_obs().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.number_of_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid._state\n",
    "y, x = grid._state\n",
    "value = y*grid._layout.shape[1] + x\n",
    "feat = np.zeros(grid.number_of_states)\n",
    "feat[value-1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "from lib import plotting\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to convert a state to a featurizes represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.079,  0.095,  0.137, -0.009, -0.027,  0.094, -0.049,  0.096,\n",
       "        0.121,  0.134,  0.041,  0.06 ,  0.083,  0.031, -0.129, -0.135,\n",
       "        0.049,  0.134,  0.087, -0.14 , -0.015, -0.036,  0.029, -0.127,\n",
       "       -0.079,  0.139,  0.114, -0.085, -0.05 ,  0.052, -0.006,  0.135,\n",
       "        0.136,  0.052, -0.107, -0.061,  0.134,  0.102,  0.123,  0.141,\n",
       "       -0.08 , -0.054,  0.069,  0.027, -0.076, -0.136,  0.053, -0.009,\n",
       "       -0.105, -0.134, -0.023,  0.141, -0.133, -0.111, -0.132, -0.141,\n",
       "        0.135, -0.141, -0.092,  0.115,  0.068,  0.079,  0.005, -0.11 ,\n",
       "        0.133, -0.041, -0.109,  0.033,  0.118, -0.012,  0.138,  0.003,\n",
       "       -0.132, -0.132, -0.011, -0.126,  0.029,  0.122,  0.065, -0.13 ,\n",
       "       -0.137, -0.141,  0.09 , -0.077, -0.122,  0.049,  0.092,  0.063,\n",
       "       -0.141, -0.106, -0.015, -0.112, -0.076,  0.141, -0.043, -0.042,\n",
       "       -0.115, -0.036,  0.061, -0.12 , -0.067, -0.138, -0.051, -0.018,\n",
       "        0.046,  0.063,  0.117, -0.099, -0.116, -0.14 ,  0.109,  0.061,\n",
       "        0.084,  0.049,  0.071, -0.105,  0.04 , -0.126,  0.053,  0.083,\n",
       "       -0.075,  0.021, -0.136,  0.136,  0.129, -0.09 , -0.012,  0.087,\n",
       "        0.113,  0.136, -0.128,  0.059,  0.034,  0.141, -0.007,  0.106,\n",
       "       -0.045,  0.09 ,  0.081,  0.11 ,  0.141,  0.138, -0.141, -0.02 ,\n",
       "        0.131,  0.099, -0.055, -0.136,  0.141,  0.019, -0.093,  0.141,\n",
       "        0.125,  0.12 ,  0.131, -0.027, -0.096,  0.129,  0.108,  0.015,\n",
       "       -0.064,  0.108, -0.013, -0.066, -0.135,  0.096,  0.141,  0.025,\n",
       "        0.094,  0.05 ,  0.   , -0.096, -0.139, -0.141, -0.005,  0.061,\n",
       "        0.141,  0.023,  0.103, -0.057, -0.061, -0.136, -0.117, -0.094,\n",
       "        0.114,  0.137,  0.104,  0.133,  0.117, -0.137, -0.095, -0.094,\n",
       "        0.111, -0.06 ,  0.077, -0.141, -0.139, -0.095, -0.14 ,  0.141,\n",
       "        0.129,  0.025, -0.062,  0.073,  0.017,  0.021, -0.008, -0.137,\n",
       "        0.057,  0.134,  0.067, -0.129, -0.136,  0.016, -0.14 , -0.038,\n",
       "        0.099,  0.115,  0.052,  0.14 ,  0.121, -0.079, -0.135,  0.087,\n",
       "       -0.118, -0.064, -0.139,  0.14 , -0.109,  0.031, -0.055,  0.139,\n",
       "        0.077, -0.002,  0.085,  0.09 , -0.01 , -0.129,  0.064, -0.05 ,\n",
       "        0.022,  0.139,  0.139,  0.024,  0.133, -0.124, -0.112, -0.019,\n",
       "       -0.037, -0.112, -0.114,  0.135, -0.132,  0.041,  0.097,  0.131,\n",
       "        0.127,  0.076, -0.054,  0.125, -0.051, -0.137,  0.098, -0.019,\n",
       "        0.104,  0.1  ,  0.033, -0.032, -0.001,  0.012, -0.124, -0.13 ,\n",
       "        0.141, -0.02 , -0.106,  0.135,  0.009,  0.127,  0.106, -0.013,\n",
       "        0.025,  0.138,  0.14 , -0.011, -0.141, -0.081,  0.117,  0.126,\n",
       "       -0.119, -0.072,  0.123, -0.048, -0.043, -0.103, -0.139,  0.072,\n",
       "       -0.058,  0.033, -0.137, -0.008,  0.068, -0.063, -0.115, -0.141,\n",
       "        0.116, -0.102,  0.025, -0.136, -0.025, -0.14 ,  0.122, -0.056,\n",
       "        0.027, -0.039, -0.086,  0.07 ,  0.047,  0.05 , -0.102, -0.075,\n",
       "       -0.128,  0.087,  0.106, -0.11 , -0.1  , -0.131,  0.07 , -0.092,\n",
       "        0.126, -0.006,  0.008, -0.088, -0.131,  0.111, -0.068, -0.141,\n",
       "        0.08 ,  0.081, -0.097, -0.042,  0.14 ,  0.125,  0.036, -0.093,\n",
       "       -0.116, -0.14 ,  0.008,  0.047,  0.058, -0.139, -0.088,  0.008,\n",
       "       -0.141, -0.141,  0.019, -0.038, -0.107,  0.101,  0.037, -0.096,\n",
       "        0.033,  0.079,  0.028,  0.052, -0.114,  0.121, -0.124,  0.114,\n",
       "        0.132,  0.139, -0.132, -0.071,  0.037, -0.039, -0.141,  0.113,\n",
       "       -0.052, -0.099, -0.11 ,  0.141, -0.043,  0.131,  0.136, -0.139,\n",
       "       -0.093,  0.071,  0.091,  0.094,  0.126, -0.06 ,  0.013, -0.001,\n",
       "       -0.121, -0.049,  0.14 ,  0.053, -0.123, -0.138,  0.024,  0.012])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import gym\n",
    "\n",
    "# Hyper Parameters\n",
    "STATE_DIM = 4\n",
    "ACTION_DIM = 2\n",
    "STEP = 2000\n",
    "SAMPLE_NUMS = 30\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,action_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.log_softmax(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "def roll_out(actor_network,task,sample_nums,value_network,init_state):\n",
    "    #task.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    is_done = False\n",
    "    final_r = 0\n",
    "    state = init_state\n",
    "\n",
    "    for j in range(sample_nums):\n",
    "        states.append(state)\n",
    "        log_softmax_action = actor_network(Variable(torch.Tensor([state])))\n",
    "        softmax_action = torch.exp(log_softmax_action)\n",
    "        action = np.random.choice(ACTION_DIM,p=softmax_action.cpu().data.numpy()[0])\n",
    "        one_hot_action = [int(k == action) for k in range(ACTION_DIM)]\n",
    "        next_state,reward,done,_ = task.step(action)\n",
    "        #fix_reward = -10 if done else 1\n",
    "        actions.append(one_hot_action)\n",
    "        rewards.append(reward)\n",
    "        final_state = next_state\n",
    "        state = next_state\n",
    "        if done:\n",
    "            is_done = True\n",
    "            state = task.reset()\n",
    "            break\n",
    "    if not is_done:\n",
    "        final_r = value_network(Variable(torch.Tensor([final_state]))).cpu().data.numpy()\n",
    "\n",
    "    return states,actions,rewards,final_r,state\n",
    "\n",
    "def discount_reward(r, gamma,final_r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = final_r\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # init a task generator for data fetching\n",
    "    task = gym.make(\"CartPole-v0\")\n",
    "    init_state = task.reset()\n",
    "\n",
    "    # init value network\n",
    "    value_network = ValueNetwork(input_size = STATE_DIM,hidden_size = 40,output_size = 1)\n",
    "    value_network_optim = torch.optim.Adam(value_network.parameters(),lr=0.01)\n",
    "\n",
    "    # init actor network\n",
    "    actor_network = ActorNetwork(STATE_DIM,40,ACTION_DIM)\n",
    "    actor_network_optim = torch.optim.Adam(actor_network.parameters(),lr = 0.01)\n",
    "\n",
    "    steps =[]\n",
    "    task_episodes =[]\n",
    "    test_results =[]\n",
    "\n",
    "    for step in range(STEP):\n",
    "        states,actions,rewards,final_r,current_state = roll_out(actor_network,task,SAMPLE_NUMS,value_network,init_state)\n",
    "        init_state = current_state\n",
    "        actions_var = Variable(torch.Tensor(actions).view(-1,ACTION_DIM))\n",
    "        states_var = Variable(torch.Tensor(states).view(-1,STATE_DIM))\n",
    "\n",
    "        # train actor network\n",
    "        actor_network_optim.zero_grad()\n",
    "        log_softmax_actions = actor_network(states_var)\n",
    "        vs = value_network(states_var).detach()\n",
    "        # calculate qs\n",
    "        qs = Variable(torch.Tensor(discount_reward(rewards,0.99,final_r)))\n",
    "\n",
    "        advantages = qs - vs\n",
    "        actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* advantages)\n",
    "        actor_network_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(actor_network.parameters(),0.5)\n",
    "        actor_network_optim.step()\n",
    "\n",
    "        # train value network\n",
    "        value_network_optim.zero_grad()\n",
    "        target_values = qs\n",
    "        values = value_network(states_var)\n",
    "        criterion = nn.MSELoss()\n",
    "        value_network_loss = criterion(values,target_values)\n",
    "        value_network_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(value_network.parameters(),0.5)\n",
    "        value_network_optim.step()\n",
    "\n",
    "        # Testing\n",
    "        if (step + 1) % 50== 0:\n",
    "                result = 0\n",
    "                test_task = gym.make(\"CartPole-v0\")\n",
    "                for test_epi in range(10):\n",
    "                    state = test_task.reset()\n",
    "                    for test_step in range(200):\n",
    "                        softmax_action = torch.exp(actor_network(Variable(torch.Tensor([state]))))\n",
    "                        #print(softmax_action.data)\n",
    "                        action = np.argmax(softmax_action.data.numpy()[0])\n",
    "                        next_state,reward,done,_ = test_task.step(action)\n",
    "                        result += reward\n",
    "                        state = next_state\n",
    "                        if done:\n",
    "                            break\n",
    "                print(\"step:\",step+1,\"test result:\",result/10.0)\n",
    "                steps.append(step+1)\n",
    "                test_results.append(result/10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# init a task generator for data fetching\n",
    "task = gym.make(\"CartPole-v0\")\n",
    "init_state = task.reset()\n",
    "\n",
    "# init value network\n",
    "value_network = ValueNetwork(input_size = STATE_DIM,hidden_size = 40,output_size = 1)\n",
    "value_network_optim = torch.optim.Adam(value_network.parameters(),lr=0.01)\n",
    "\n",
    "# init actor network\n",
    "actor_network = ActorNetwork(STATE_DIM,40,ACTION_DIM)\n",
    "actor_network_optim = torch.optim.Adam(actor_network.parameters(),lr = 0.01)\n",
    "\n",
    "steps =[]\n",
    "task_episodes =[]\n",
    "test_results =[]\n",
    "\n",
    "softmax_action = torch.exp(actor_network(Variable(torch.Tensor([state]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5225  0.4775\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01 ,  0.039,  0.028, -0.033])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
