{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "#Now to implement q learning and variants on the above market environment\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "#from lib.utes import run_experiment, plot_values, plot_greedy_policy, \\\n",
    "#                        plot_action_values, random_policy, plot_state_value\n",
    "\n",
    "#from lib.envs.grid import Grid, FeatureGrid, AltGrid\n",
    "#from TD import  GeneralQ, ExperienceQ, DynaQ #, FeatureExperienceQ, FeatureDynaQ\n",
    "#from FA.model import TabularModel #, LinearModel\n",
    "#from Tabular import ExpTabAgent\n",
    "\n",
    "from lib.envs.market import Market\n",
    "from lib.graphs import make_baseline_graphs, make_agent_graphs, plot_sample_paths, plot_disc_utility, \\\n",
    "                        plot_mv_equiv, plot_const_step\n",
    "    \n",
    "from lib.sim_prices import make_prices, run_stepsim_fixedprop\n",
    "from lib.execute_strat import execute_strat\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Important to store and set the different parameters\n",
    "perhaps pass in a dictionary of parameters each time to avoid silly errors\n",
    "\n",
    "Experiment 1: Setup\n",
    "mu 0.11, rf 0.10, sigma 0.028\n",
    "M=20, T=1.0, utes = 15, S0=1, B0=1, X0=100, wealth = X0, merton 12.75, \n",
    "u_star = np.linspace(-10,25, utes)\n",
    "\n",
    "sims 300,000\n",
    "kappa 0.008\n",
    "\n",
    "eps greedy 0.1, learning rate 0.1, gamma 0.95\n",
    "All the different models\n",
    "do risk free in build up and log utility based results, also show extreme risk aversion\n",
    "\n",
    "(do we vary kappa?)\n",
    "\n",
    "\n",
    "Experiment 2: Higher volatility - can it be learned? Show the problems...perhaps its a trade off\n",
    "of number of episodes\n",
    "\n",
    "mu 0.11, rf 0.10, sigma 0.1\n",
    "M=20, T=1.0, utes = 15, S0=1, B0=1, X0=100, wealth = X0, merton 12.75, \n",
    "u_star = np.linspace(-1,1.5, utes)\n",
    "\n",
    "sims 300,000\n",
    "kappa 0.007 - trying different values for kappa\n",
    "\n",
    "eps greedy 0.1, learning rate 0.1, gamma 0.95\n",
    "\n",
    "\n",
    "Experiment 3: Another utility curve as per experiment 1\n",
    "One I can parameterise with risk aversion and show the RL learns it...\n",
    "\n",
    "\n",
    "Experiment 4: Scaling up - alternative RL methods\n",
    "\n",
    "\n",
    "Experiment 5: Different price processes - maybe trade OU first?\n",
    "Then see if we can beat merton with a non gbm process?\n",
    "\n",
    "\n",
    "Chapters ?\n",
    "Intro\n",
    "Risk Neutral and Risk Averse\n",
    "Utilities\n",
    "Single Period Investing\n",
    "The Merton Problem - Multi Period Optimality\n",
    "Stochastic Optimal Control - The solution\n",
    "Reinforcement Learning\n",
    "Basic Methods\n",
    "Problem Setting for RL - myopic investing\n",
    "RL difficulties\n",
    "Some examples - vols, means...\n",
    "Learning Merton Experimental setting - from Myopic to mean variance equivalance\n",
    "Low Vol results (diff RL algos)\n",
    "High Vol - trade off with episodes?\n",
    "Increasing the difference between rf and mu\n",
    "Different Utility curves - learning increasing risk aversion\n",
    "Scaling up - RL\n",
    "Continuous setting\n",
    "Realistic price processes - beating Merton\n",
    "Gaussian Processes\n",
    "Reducing sample numbers with a semi-parametric approach (hypothesising a set of prior kernels\n",
    "rather than learning purely from the data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "mu =0.10 #save these paramters mu 0.11, rf 0.10, sigma 0.028\n",
    "rf = 0.02\n",
    "M = 50 #so 20 time periods or 50 still works...just longer to train and run, u_star = np.linspace(-10,25, utes)\n",
    "\n",
    "time_periods = M\n",
    "T = 1.0\n",
    "dt = T/M\n",
    "utes = 25\n",
    "#u_star = np.linspace(-10,25, utes) #this is specific to the above parameters\n",
    "\n",
    "kappa = 0.007\n",
    "\n",
    "\n",
    "#We will always start with a stock price of 1, bond price of 1, and a time period which will be subdivided\n",
    "#wealth starts at 100\n",
    "S0 = 1\n",
    "B0 = 1\n",
    "X0 = 100\n",
    "wealth = X0\n",
    "\n",
    "lowvol =False\n",
    "\n",
    "#file = 'LogUteNoisy1epi3000000er8kappa0.0.npy'\n",
    "if lowvol:\n",
    "    u_star = np.linspace(-10,25, utes)\n",
    "    sigma = 0.028\n",
    "else:\n",
    "    u_star = np.linspace(0,2, utes)\n",
    "    sigma = 0.35\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the ritter paper he then instead of calculating the mean variance equivalent\n",
    "#form at the end, turned this into an incremental reward. He did this for trading an OU\n",
    "#process showing q learning could find an arbitrage - the step by step is dx - kappa*(dx**2)\n",
    "\n",
    "#Using the same set up I hoped to be able to approach mertons problem with RL\n",
    "#the agent won't know the price process, it will have 15 different investment possibilities\n",
    "#at each time, it will know its wealth level, and have an action of how much to invest.\n",
    "#each step this order will be executed by the market agent and it will receive a reward. \n",
    "\n",
    "#Difficulty one - the agent has no control of much of the next state...it is effectively choosing\n",
    "#between different probability distributions over wealth, the price process is noisy and independent of\n",
    "#it, thus the action doesn't control much of the next environment.\n",
    "\n",
    "#Difficulty two - even a low vol levels, the rewards are noisy and can be misleading. The agent\n",
    "#does not know that 'the answer' is a constant investment ratio, it must explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The merton ratio leads to a constant ratio for GBM log utility\n",
    "#note this is a very high leverage because of the low vol\n",
    "#I did this due to difficulties getting RL to learn anything \n",
    "#and we can play around with how to solve this\n",
    "\n",
    "merton_ratio = (mu-rf)/sigma**2\n",
    "merton_ratio #the ratio to invest\n",
    "\n",
    "merton_value = np.log(X0) + (rf + ((mu-rf)/2*sigma**2))\n",
    "\n",
    "print(merton_ratio, merton_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log(X0) + (rf + ((mu-rf)**2/2*sigma**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ritters paper looks at mean variance equivalence distributions\n",
    "#he can thus convert with a parameter kappa the above utility to \n",
    "#a mean variance trade off...this is possible for all mean variance equiv distn\n",
    "#and upward sloping utility function (of which log is)\n",
    "\n",
    "def mv_equiv(kappa, mean, variance):\n",
    "    return mean - (kappa/2)*variance\n",
    "\n",
    "merton_ratio = (mu-rf)/sigma**2\n",
    "best_action = np.argmin(np.abs(u_star-merton_ratio))\n",
    "print(best_action)\n",
    "print(u_star[best_action])\n",
    "u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims =300000\n",
    "S, B, utility, means, variances = make_prices(mu, sigma, rf, utes, time_periods, dt, X0, B0,S0, u_star, num_sims)\n",
    "\n",
    "#if we know the answer is log utility and \n",
    "#we know that we have gbm then we can simulate to\n",
    "#see if our best action is close to the optimal ratio\n",
    "#according to merton's formula\n",
    "\n",
    "#we need this to compare the merton strategy to our learned agent\n",
    "best_action = np.argmax(utility)\n",
    "utility[best_action]\n",
    "u_star[best_action] #12.5 is the closest we have to merton which is 12.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this gives mean variance equivalent end of episode values\n",
    "kappa = 0.008\n",
    "vals = mv_equiv(kappa, means, variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a reward step by step simulation with fixed investment levels to check this looks\n",
    "#similar to the utility set up\n",
    "\n",
    "episodes = 200000\n",
    "kappa = 0.008\n",
    "mean_rewards, mean_utes, all_wealth = run_stepsim_fixedprop(kappa, mu, sigma, rf,\n",
    "                                                            u_star, Market, wealth, episodes=200000, time_periods=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_paths(S, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disc_utility(u_star,utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mv_equiv(u_star, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_const_step(u_star,mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####be sure to calibrate kappa in the same way as the training for the agent\n",
    "#kappa = 0.006\n",
    "\n",
    "#run the various baseline graphs pre agent\n",
    "utilities_test_rand, rewards_test_rand, step_rew_rand, wealth_test_rand = execute_strat(kappa,mu, rf, sigma, utes,u_star,best_action, 'Random', \n",
    "                                                                      time_periods=time_periods, wealth=wealth)\n",
    "utilities_test_best, rewards_test_best, step_rew_best, wealth_test_best = execute_strat(kappa, mu, rf, sigma, utes, u_star, best_action,'Merton', \n",
    "                                                                      time_periods=time_periods, wealth=wealth)\n",
    "\n",
    "#block_utilities_test_rand = np.mean(np.array(utilities_test_rand).reshape(1000,-1),0)\n",
    "#block_utilities_test_best = np.mean(np.array(utilities_test_best).reshape(1000,-1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_baseline_graphs(utilities_test_rand,utilities_test_best,\n",
    "                         rewards_test_rand,rewards_test_best, step_rew_rand, step_rew_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now with the trained agent\n",
    "import os\n",
    "path = '/Users/johngoodacre/Desktop/RLMerton/rl/td/q_tables/models'\n",
    "files = os.listdir(path)\n",
    "\n",
    "#Q = np.load('q_tables/prior_runs/best_q_so_far.npy')\n",
    "#Q = np.load('q_tables/models/2epi3000000er8.npy')\n",
    "\n",
    "#for file in files:\n",
    "#    Q = np.load(path+'/'+file)\n",
    "#    print(file)\n",
    "\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa1.0.npy') = this gave some v nice results...\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa2.0.npy') and this was very bad??\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa3.0.npy') this looked good also\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa4.0.npy') very strong\n",
    "\n",
    "\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa9.0.npy') not so convincing\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa10.0.npy') bad\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa11.0.npy') not great\n",
    "#Q = np.load('q_tables/models/Noisy2epi3000000er8kappa13.0.npy') good risk averse\n",
    "\n",
    "#Q = np.load('q_tables/models/LowVol2epi3000000er8kappa0.0.npy') #this is good...risk neutrality demo\n",
    "#Q = np.load('q_tables/models/LowVol2epi3000000er8kappa9.0.npy') not convincing\n",
    "#Q = np.load('q_tables/models/LogUteLowVol2epi3000000er8kappa8.0.npy') using utes - looks good\n",
    "\n",
    "#LogUteNoisy1epi3000000er8kappa0.0.npy velly nice\n",
    "\n",
    "\n",
    "#Q = np.load('q_tables/models/LogUteNoisy1epi3000000er8kappa1.0.npy') #this works\n",
    "\n",
    "\n",
    "#path = '/Users/johngoodacre/Desktop/RLMerton/rl/td/q_tables/models'\n",
    "#files = os.listdir(path)\n",
    "\n",
    "results = []\n",
    "\n",
    "#Q = np.load('q_tables/diffmodels/LogUteNoisy1epi3000000er8kappa1.0.npy')\n",
    "Q = np.load('q_tables/diffmodels/LogUteNoisy3epi3000000er8kappa130.0.npy')\n",
    "\n",
    "\n",
    "path = '/Users/johngoodacre/Desktop/RLMerton/rl/td/q_tables/diffmodels/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    if 'LogUtestudt' in file:\n",
    "        print(file)\n",
    "\n",
    "        Q = np.load(path+'/'+file)\n",
    "        utilities_test, rewards_test, step_rew_test, wealth_test = execute_strat(kappa, mu, rf, sigma, utes, \n",
    "                                        u_star,best_action, 'Agent', q_values=Q, \n",
    "                                        time_periods=time_periods, wealth=wealth)\n",
    "\n",
    "\n",
    "        res = make_agent_graphs(rewards_test_best, rewards_test_rand, rewards_test,\n",
    "              utilities_test_best, utilities_test_rand, utilities_test,\n",
    "             wealth_test_rand, wealth_test_best, wealth_test)\n",
    "        results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in res:\n",
    "    print(np.std(r))\n",
    "    #print(np.mean(r)+np.log(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
