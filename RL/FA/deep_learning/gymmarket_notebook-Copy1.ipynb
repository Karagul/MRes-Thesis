{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "#Now to implement q learning and variants on the above market environment\n",
    "import sys\n",
    "if \"../../\" not in sys.path:\n",
    "  sys.path.append(\"../../\") \n",
    "\n",
    "from TD.TD import GeneralQ #, ExperienceQ, DynaQ \n",
    "#from FA.model import TabularModel \n",
    "#from TD.Tabular import ExpTabAgent\n",
    "#from lib.envs.market import Market\n",
    "\n",
    "import collections\n",
    "import ptan\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Market(gym.Env):\n",
    "    def __init__(self, kappa, episodes, time_periods, mu, r, sigma, inv_range):\n",
    "\n",
    "        self.episodes = episodes\n",
    "        self.time_periods = time_periods\n",
    "        self.epi = 0\n",
    "        self.t = 0\n",
    "\n",
    "        self.inv_range = inv_range\n",
    "\n",
    "        self.mu = mu\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self._start_wealth = 100.0\n",
    "        self.kappa = kappa\n",
    "\n",
    "        self.action_space = spaces.Discrete(len(inv_range))\n",
    "        self.observation_space = spaces.Box(0,120, np.array([2]))\n",
    "        \n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.S, self.B, self.dS, self.dB = self.generate_price_series()\n",
    "        self.wealth = self._start_wealth\n",
    "        self.state = (int(self.S[self.t,self.epi]*100),int(self._start_wealth/10))\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        price_state, wealth_state = self.state\n",
    "        \n",
    "        prop = self.inv_range[action]\n",
    "\n",
    "        discount = 0.99\n",
    "\n",
    "        if self.t < self.time_periods:\n",
    "            NB = (1-prop)*self.wealth/self.B[self.t]\n",
    "            NS = prop*self.wealth/self.S[self.t, self.epi]\n",
    "\n",
    "            dX = NB*self.dB[self.t] +NS*self.dS[self.t,self.epi]\n",
    "            reward = dX - (self.kappa/2)*(dX**2)  \n",
    "            self.wealth += dX\n",
    "\n",
    "            done = False\n",
    "\n",
    "            self.wealth_state = int(self.wealth/10)\n",
    "            self.t += 1\n",
    "            \n",
    "            new_state = (int(self.S[self.t,self.epi]*100), self.wealth_state)\n",
    "            self.state = new_state\n",
    "            final_wealth = 0.0\n",
    "            \n",
    "        else:\n",
    "            #reached the end of episode...\n",
    "            self.t = 0\n",
    "            self.epi = 0 #+=1\n",
    "\n",
    "            reward = 0.0\n",
    "            dX = 0.0\n",
    "            final_wealth = self.wealth\n",
    "            done = True\n",
    "\n",
    "            _ = self.reset()\n",
    "\n",
    "        return np.array(self.state), reward, done, final_wealth\n",
    "\n",
    "    def generate_price_series(self):\n",
    "        I = 1 #self.episodes\n",
    "        M = self.time_periods\n",
    "\n",
    "        S0 = 1\n",
    "        B0 = 1\n",
    "        T = 1.0\n",
    "        dt = T/M\n",
    "\n",
    "        mu = self.mu\n",
    "        r = self.r\n",
    "        sigma = self.sigma\n",
    "\n",
    "        S = np.zeros((M+1,I))\n",
    "        dS = np.zeros((M,I))\n",
    "        dB = np.zeros(M)\n",
    "\n",
    "        B = np.zeros(M+1)\n",
    "        B[0] = B0\n",
    "        S[0] = S0\n",
    "\n",
    "        for t in range(1, M+1):\n",
    "            z = np.random.standard_normal(I)\n",
    "            #df = 10\n",
    "            #z = np.random.standard_t(df,I)\n",
    "            S[t] = S[t-1]*np.exp((mu-0.5*sigma**2)*dt + sigma*math.sqrt(dt)*z)\n",
    "            B[t] = B[t-1]*np.exp(r*dt)\n",
    "\n",
    "        for t in range(1,M):\n",
    "            dS[t] = S[t+1] - S[t]\n",
    "            dB[t] = B[t+1] - B[t]\n",
    "\n",
    "        return S, B, dS, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kappa = 0.008\n",
    "episodes = 3000000\n",
    "time_periods = 20\n",
    "mu =0.10\n",
    "rf = 0.02\n",
    "sigma = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "episodes = episodes #500k worked well? as did 1m\n",
    "\n",
    "utes = 15\n",
    "u_star = np.linspace(0,2, utes)\n",
    "\n",
    "Mark = Market(kappa, episodes, time_periods, mu, rf, sigma, u_star) #parameters from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = Mark\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0],256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1)\n",
    "agent = ptan.agent.DQNAgent(net, action_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array([env.reset()], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), [None])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.FloatTensor but found type torch.DoubleTensor for argument #4 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-39e5fbfa4fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexp_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mstates_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstates_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mstates_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_agent_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/ptan/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, states, agent_states)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mq_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.DoubleTensor for argument #4 'mat1'"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=1)\n",
    "it = iter(exp_source)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env #gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()[1]\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        \n",
    "        print(reward)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        \n",
    "        self.state = self.state[1]\n",
    "        new_state = new_state[1]\n",
    "        return (old_state, action, reward, new_state)\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        \n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action      \n",
    "        if np.random.random() < 0.1:\n",
    "            best_action = self.env.action_space.sample()\n",
    "            \n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        \n",
    "        new_val = r + GAMMA * best_v\n",
    "        old_val = self.values[(s, a)]\n",
    "        \n",
    "        #print(best_v,r)\n",
    "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()[1]\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            #print(reward)\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state[1]\n",
    "        \n",
    "        #print(total_reward)\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Best reward updated 0.000 -> 4.325\n",
      "0.0\n",
      "Best reward updated 4.325 -> 4.640\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Best reward updated 4.640 -> 5.106\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Best reward updated 5.106 -> 5.404\n",
      "0.0\n",
      "Best reward updated 5.404 -> 6.560\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Best reward updated 6.560 -> 7.834\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-41619cdf5d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mTEST_EPISODES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-4642eb90fe05>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value_and_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-a2fae5d0cb43>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_price_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m#plt.plot(self.S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwealth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_wealth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-a2fae5d0cb43>\u001b[0m in \u001b[0;36mgenerate_price_series\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m#df = 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m#z = np.random.standard_t(df,I)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "\n",
    "test_env = env\n",
    "agent = Agent(test_env)\n",
    "writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "for i_episode in range(num_episodes-1):\n",
    "    iter_no += 1\n",
    "    s, a, r, next_s = agent.sample_env()\n",
    "    \n",
    "    agent.value_update(s, a, r, next_s)\n",
    "\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {(5, 0): 0.0,\n",
       "             (5, 1): 0.0,\n",
       "             (5, 2): 0.0,\n",
       "             (5, 3): 0.0,\n",
       "             (5, 4): 0.0,\n",
       "             (5, 5): 0.0,\n",
       "             (5, 6): 0.0,\n",
       "             (5, 7): 0.0,\n",
       "             (5, 8): 0.0,\n",
       "             (5, 9): 0.0,\n",
       "             (5, 10): 0.0,\n",
       "             (5, 11): 0.0,\n",
       "             (5, 12): 0.0,\n",
       "             (5, 13): 0.0,\n",
       "             (5, 14): 0.0,\n",
       "             (6, 0): 0.0,\n",
       "             (6, 1): 0.0,\n",
       "             (6, 2): 0.0,\n",
       "             (6, 3): 0.0,\n",
       "             (6, 4): 0.0,\n",
       "             (6, 5): 0.0,\n",
       "             (6, 6): 0.0,\n",
       "             (6, 7): 0.0,\n",
       "             (6, 8): 0.0,\n",
       "             (6, 9): 0.0,\n",
       "             (6, 10): 0.0,\n",
       "             (6, 11): 0.0,\n",
       "             (6, 12): 0.0,\n",
       "             (6, 13): 0.0,\n",
       "             (6, 14): 0.0,\n",
       "             (7, 0): 0.0,\n",
       "             (7, 1): 0.0,\n",
       "             (7, 2): 0.0,\n",
       "             (7, 3): 0.0,\n",
       "             (7, 4): 0.0,\n",
       "             (7, 5): 0.0,\n",
       "             (7, 6): 0.0,\n",
       "             (7, 7): 0.0,\n",
       "             (7, 8): 0.0,\n",
       "             (7, 9): 0.0,\n",
       "             (7, 10): 0.0,\n",
       "             (7, 11): 0.0,\n",
       "             (7, 12): 0.0,\n",
       "             (7, 13): 0.0,\n",
       "             (7, 14): 0.0,\n",
       "             (8, 0): 0.0,\n",
       "             (8, 1): 0.0,\n",
       "             (8, 2): 0.0,\n",
       "             (8, 3): 0.0,\n",
       "             (8, 4): 0.0,\n",
       "             (8, 5): 0.0,\n",
       "             (8, 6): 0.0,\n",
       "             (8, 7): 0.0,\n",
       "             (8, 8): 0.0,\n",
       "             (8, 9): 0.0,\n",
       "             (8, 10): 0.0,\n",
       "             (8, 11): 0.0,\n",
       "             (8, 12): 0.0,\n",
       "             (8, 13): 0.0,\n",
       "             (8, 14): 0.0,\n",
       "             (9, 0): 0.0,\n",
       "             (9, 1): 0.0,\n",
       "             (9, 2): 0.0,\n",
       "             (9, 3): 0.0,\n",
       "             (9, 4): 0.0,\n",
       "             (9, 5): 0.0,\n",
       "             (9, 6): 0.0,\n",
       "             (9, 7): 0.0,\n",
       "             (9, 8): 0.0,\n",
       "             (9, 9): 0.0,\n",
       "             (9, 10): 0.0,\n",
       "             (9, 11): 0.0,\n",
       "             (9, 12): 0.0,\n",
       "             (9, 13): 0.0,\n",
       "             (9, 14): 0.0,\n",
       "             (10, 0): 0.0,\n",
       "             (10, 1): 0.0,\n",
       "             (10, 2): 0.0,\n",
       "             (10, 3): 0.0,\n",
       "             (10, 4): 0.0,\n",
       "             (10, 5): 0.0,\n",
       "             (10, 6): 0.0,\n",
       "             (10, 7): 0.0,\n",
       "             (10, 8): 0.0,\n",
       "             (10, 9): 0.0,\n",
       "             (10, 10): 0.0,\n",
       "             (10, 11): 0.0,\n",
       "             (10, 12): 0.0,\n",
       "             (10, 13): 0.0,\n",
       "             (10, 14): 0.0,\n",
       "             (11, 0): 0.0,\n",
       "             (11, 1): 0.0,\n",
       "             (11, 2): 0.0,\n",
       "             (11, 3): 0.0,\n",
       "             (11, 4): 0.0,\n",
       "             (11, 5): 0.0,\n",
       "             (11, 6): 0.0,\n",
       "             (11, 7): 0.0,\n",
       "             (11, 8): 0.0,\n",
       "             (11, 9): 0.0,\n",
       "             (11, 10): 0.0,\n",
       "             (11, 11): 0.0,\n",
       "             (11, 12): 0.0,\n",
       "             (11, 13): 0.0,\n",
       "             (11, 14): 0.0,\n",
       "             (12, 0): 0.0,\n",
       "             (12, 1): 0.0,\n",
       "             (12, 2): 0.0,\n",
       "             (12, 3): 0.0,\n",
       "             (12, 4): 0.0,\n",
       "             (12, 5): 0.0,\n",
       "             (12, 6): 0.0,\n",
       "             (12, 7): 0.0,\n",
       "             (12, 8): 0.0,\n",
       "             (12, 9): 0.0,\n",
       "             (12, 10): 0.0,\n",
       "             (12, 11): 0.0,\n",
       "             (12, 12): 0.0,\n",
       "             (12, 13): 0.0,\n",
       "             (12, 14): 0.0,\n",
       "             (13, 0): 0.0,\n",
       "             (13, 1): 0.0,\n",
       "             (13, 2): 0.0,\n",
       "             (13, 3): 0.0,\n",
       "             (13, 4): 0.0,\n",
       "             (13, 5): 0.0,\n",
       "             (13, 6): 0.0,\n",
       "             (13, 7): 0.0,\n",
       "             (13, 8): 0.0,\n",
       "             (13, 9): 0.0,\n",
       "             (13, 10): 0.0,\n",
       "             (13, 11): 0.0,\n",
       "             (13, 12): 0.0,\n",
       "             (13, 13): 0.0,\n",
       "             (13, 14): 0.0,\n",
       "             (14, 0): 0.0,\n",
       "             (14, 1): 0.0,\n",
       "             (14, 2): 0.0,\n",
       "             (14, 3): 0.0,\n",
       "             (14, 4): 0.0,\n",
       "             (14, 5): 0.0,\n",
       "             (14, 6): 0.0,\n",
       "             (14, 7): 0.0,\n",
       "             (14, 8): 0.0,\n",
       "             (14, 9): 0.0,\n",
       "             (14, 10): 0.0,\n",
       "             (14, 11): 0.0,\n",
       "             (14, 12): 0.0,\n",
       "             (14, 13): 0.0,\n",
       "             (14, 14): 0.0,\n",
       "             (15, 0): 0.0,\n",
       "             (15, 1): 0.0,\n",
       "             (15, 2): 0.0,\n",
       "             (15, 3): 0.0,\n",
       "             (15, 4): 0.0,\n",
       "             (15, 5): 0.0,\n",
       "             (15, 6): 0.0,\n",
       "             (15, 7): 0.0,\n",
       "             (15, 8): 0.0,\n",
       "             (15, 9): 0.0,\n",
       "             (15, 10): 0.0,\n",
       "             (15, 11): 0.0,\n",
       "             (15, 12): 0.0,\n",
       "             (15, 13): 0.0,\n",
       "             (15, 14): 0.0,\n",
       "             (16, 0): 0.0,\n",
       "             (16, 1): 0.0,\n",
       "             (16, 2): 0.0,\n",
       "             (16, 3): 0.0,\n",
       "             (16, 4): 0.0,\n",
       "             (16, 5): 0.0,\n",
       "             (16, 6): 0.0,\n",
       "             (16, 7): 0.0,\n",
       "             (16, 8): 0.0,\n",
       "             (16, 9): 0.0,\n",
       "             (16, 10): 0.0,\n",
       "             (16, 11): 0.0,\n",
       "             (16, 12): 0.0,\n",
       "             (16, 13): 0.0,\n",
       "             (16, 14): 0.0,\n",
       "             (17, 0): 0.0,\n",
       "             (17, 1): 0.0,\n",
       "             (17, 2): 0.0,\n",
       "             (17, 3): 0.0,\n",
       "             (17, 4): 0.0,\n",
       "             (17, 5): 0.0,\n",
       "             (17, 6): 0.0,\n",
       "             (17, 7): 0.0,\n",
       "             (17, 8): 0.0,\n",
       "             (17, 9): 0.0,\n",
       "             (17, 10): 0.0,\n",
       "             (17, 11): 0.0,\n",
       "             (17, 12): 0.0,\n",
       "             (17, 13): 0.0,\n",
       "             (17, 14): 0.0,\n",
       "             (18, 0): 0.0,\n",
       "             (18, 1): 0.0,\n",
       "             (18, 2): 0.0,\n",
       "             (18, 3): 0.0,\n",
       "             (18, 4): 0.0,\n",
       "             (18, 5): 0.0,\n",
       "             (18, 6): 0.0,\n",
       "             (18, 7): 0.0,\n",
       "             (18, 8): 0.0,\n",
       "             (18, 9): 0.0,\n",
       "             (18, 10): 0.0,\n",
       "             (18, 11): 0.0,\n",
       "             (18, 12): 0.0,\n",
       "             (18, 13): 0.0,\n",
       "             (18, 14): 0.0,\n",
       "             (19, 0): 0.0,\n",
       "             (19, 1): 0.0,\n",
       "             (19, 2): 0.0,\n",
       "             (19, 3): 0.0,\n",
       "             (19, 4): 0.0,\n",
       "             (19, 5): 0.0,\n",
       "             (19, 6): 0.0,\n",
       "             (19, 7): 0.0,\n",
       "             (19, 8): 0.0,\n",
       "             (19, 9): 0.0,\n",
       "             (19, 10): 0.0,\n",
       "             (19, 11): 0.0,\n",
       "             (19, 12): 0.0,\n",
       "             (19, 13): 0.0,\n",
       "             (19, 14): 0.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(kappa, episodes,):\n",
    "    \n",
    "    SARSA = False\n",
    "    double = False\n",
    "    utilities_mod = []\n",
    "    rewards_mod = []\n",
    "    wealth_episodes = []\n",
    "    rsum = 0\n",
    "\n",
    "    number_of_actions = 15 #again from earlier in the code  \n",
    "    number_of_states = 120\n",
    "\n",
    "    gamma = 0.95\n",
    "    learning_rate = 0.10\n",
    "    egreedy = 0.1\n",
    "    wealth = 100.0\n",
    "\n",
    "    start_state = int(wealth/10) \n",
    "\n",
    "    agent = GeneralQ(number_of_states, number_of_actions, start_state, SARSA, \n",
    "                 double, step_size=learning_rate) #here eps set to 0.1 anyway\n",
    "\n",
    "    \n",
    "    for i_episode in range(episodes-1):\n",
    "        \n",
    "        price_state, wealth_state = Mark.reset()\n",
    "        state = wealth_state\n",
    "        action = agent.behaviour_policy(agent._q[state])\n",
    "        #print(state)\n",
    "\n",
    "        while True:    \n",
    "\n",
    "            (price_state, new_state) , reward, done, final_wealth = Mark.step(action) #(prop, wealth))\n",
    "            action = agent.step(reward, gamma, new_state)\n",
    "            rsum += reward\n",
    "            state = new_state\n",
    "            #print(state)\n",
    "\n",
    "            if done:\n",
    "                wealth_episodes.append(final_wealth)\n",
    "                utilities_mod.append(np.log(final_wealth))\n",
    "                rewards_mod.append(rsum)\n",
    "                rsum = 0\n",
    "                #wealth = 100.0\n",
    "                break \n",
    "\n",
    "    q_name = 'gymtest'\n",
    "    filename = q_name\n",
    "    np.save(filename, agent.q_values)\n",
    "\n",
    "    print(q_name + \" last 50,000 rewards mean\",np.mean(np.array(rewards_mod)[-50000:]))\n",
    "    \n",
    "    return agent.q_values, utilities_mod, rewards_mod, wealth_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.sim_prices import make_prices\n",
    "from lib.execute_strat import execute_strat\n",
    "from lib.graphs import make_baseline_graphs, make_agent_graphs, plot_sample_paths, plot_disc_utility, \\\n",
    "                        plot_mv_equiv, plot_const_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episodes = 3000000\n",
    "Q, utilities_mod, rewards_mod, wealth_episodes = train_agent(kappa, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wealth = 100.0\n",
    "\n",
    "merton_ratio = (mu-rf)/sigma**2\n",
    "best_action = np.argmin(np.abs(u_star-merton_ratio))\n",
    "#temp change to execute strat\n",
    "utilities_test_rand, rewards_test_rand, step_rew_rand, wealth_test_rand = execute_strat(kappa,mu, \n",
    "                                                                      rf, sigma, utes,u_star,best_action, 'Random', \n",
    "                                                                      time_periods=time_periods, wealth=wealth)\n",
    "utilities_test_best, rewards_test_best, step_rew_best, wealth_test_best = execute_strat(kappa, mu, rf, sigma, \n",
    "                                                                      utes, u_star, best_action,'Merton', \n",
    "                                                                      time_periods=time_periods, wealth=wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utilities_test, rewards_test, step_rew_test, wealth_test = execute_strat(kappa, mu, rf, sigma, utes, u_star,best_action, \n",
    "                                                            'Agent', q_values=Q, \n",
    "                                                            time_periods=time_periods, wealth=wealth)\n",
    "\n",
    "results = make_agent_graphs(rewards_test_best, rewards_test_rand, rewards_test,\n",
    "                  utilities_test_best, utilities_test_rand, utilities_test,\n",
    "                         wealth_test_rand, wealth_test_best, wealth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(Q[0:40,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1 = np.load('../../TD/q_tables/models/Noisy2epi3000000er8kappa1.0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(Q1[0:40,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
