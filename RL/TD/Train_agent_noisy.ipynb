{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "#Now to implement q learning and variants on the above market environment\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "from TD import GeneralQ, ExperienceQ, DynaQ \n",
    "from FA.model import TabularModel \n",
    "from Tabular import ExpTabAgent\n",
    "from lib.envs.market import Market\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Important to store and set the different parameters\\n\\nExperiment 1: Setup\\nmu 0.11, rf 0.10, sigma 0.028\\nM=20, T=1.0, utes = 15, S0=1, B0=1, X0=100, wealth = X0, merton 12.75, \\nu_star = np.linspace(-10,25, utes)\\n\\nsims 300,000\\nkappa 0.008\\n\\neps greedy 0.1, learning rate 0.1, gamma 0.95\\nAll the different models\\ndo risk free and log utility based results, also show extreme risk aversion\\n\\n(do we vary kappa?)\\n\\n\\nExperiment 2: Higher volatility - can it be learned?\\nmu 0.11, rf 0.10, sigma 0.1\\nM=20, T=1.0, utes = 15, S0=1, B0=1, X0=100, wealth = X0, merton 12.75, \\nu_star = np.linspace(-1,1.5, utes)\\n\\nsims 300,000\\nkappa 0.007\\n\\neps greedy 0.1, learning rate 0.1, gamma 0.95\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Important to store and set the different parameters\n",
    "\n",
    "Experiment 1: Setup\n",
    "mu 0.11, rf 0.10, sigma 0.028\n",
    "M=20, T=1.0, utes = 15, S0=1, B0=1, X0=100, wealth = X0, merton 12.75, \n",
    "u_star = np.linspace(-10,25, utes)\n",
    "\n",
    "sims 300,000\n",
    "kappa 0.008\n",
    "\n",
    "eps greedy 0.1, learning rate 0.1, gamma 0.95\n",
    "All the different models\n",
    "do risk free and log utility based results, also show extreme risk aversion\n",
    "\n",
    "(do we vary kappa?)\n",
    "\n",
    "\n",
    "Experiment 2: Higher volatility - can it be learned?\n",
    "mu 0.11, rf 0.10, sigma 0.1\n",
    "M=20, T=1.0, utes = 15, S0=1, B0=1, X0=100, wealth = X0, merton 12.75, \n",
    "u_star = np.linspace(-1,1.5, utes)\n",
    "\n",
    "sims 300,000\n",
    "kappa 0.007\n",
    "\n",
    "eps greedy 0.1, learning rate 0.1, gamma 0.95\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Set parameters\\nmu =0.11 #save these paramters mu 0.11, rf 0.10, sigma 0.028\\nrf = 0.10\\nsigma = 0.028\\nsigma = 0.045\\n\\nM = 20 #so 20 time periods or 50 still works...just longer to train and run, u_star = np.linspace(-10,25, utes)\\n\\ntime_periods = M\\nT = 1.0\\ndt = T/M\\nutes = 15\\nu_star = np.linspace(-10,25, utes) #this is specific to the above parameters\\nu_star = np.linspace(-10,10, utes)\\n\\n#We will always start with a stock price of 1, bond price of 1, and a time period which will be subdivided\\n#wealth starts at 100\\nS0 = 1\\nB0 = 1\\nX0 = 100\\nwealth = X0\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "# Stage 1 of my stuff is simply get a merton ration and sample paths \n",
    "# the maths (using stochastic optimal control will be described in the thesis)\n",
    "\n",
    "#Set parameters\n",
    "mu =0.10\n",
    "rf = 0.02\n",
    "sigma = 0.35\n",
    "M = 20\n",
    "time_periods = M\n",
    "T = 1.0\n",
    "dt = T/M\n",
    "utes = 25\n",
    "#u_star = np.linspace(-1,2.5, utes) #this is specific to the above parameters\n",
    "u_star = np.linspace(0,2, utes)\n",
    "\n",
    "#We will always start with a stock price of 1, bond price of 1, and a time period which will be subdivided\n",
    "#wealth starts at 100\n",
    "S0 = 1\n",
    "B0 = 1\n",
    "X0 = 100\n",
    "wealth = X0\n",
    "kappa = 0.008\n",
    "\n",
    "###########################copied params\n",
    "\"\"\"\n",
    "#Set parameters\n",
    "mu =0.11 #save these paramters mu 0.11, rf 0.10, sigma 0.028\n",
    "rf = 0.10\n",
    "sigma = 0.028\n",
    "sigma = 0.045\n",
    "\n",
    "M = 20 #so 20 time periods or 50 still works...just longer to train and run, u_star = np.linspace(-10,25, utes)\n",
    "\n",
    "time_periods = M\n",
    "T = 1.0\n",
    "dt = T/M\n",
    "utes = 15\n",
    "u_star = np.linspace(-10,25, utes) #this is specific to the above parameters\n",
    "u_star = np.linspace(-10,10, utes)\n",
    "\n",
    "#We will always start with a stock price of 1, bond price of 1, and a time period which will be subdivided\n",
    "#wealth starts at 100\n",
    "S0 = 1\n",
    "B0 = 1\n",
    "X0 = 100\n",
    "wealth = X0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(kappa, model, episodes, exprep=0):\n",
    "    #main function to train the currently q learning tabular agents\n",
    "    \n",
    "    wealth = 100.0\n",
    "    time_periods = M #from earlier on in the code\n",
    "\n",
    "    utilities_mod = []\n",
    "    rewards_mod = []\n",
    "    wealth_episodes = []\n",
    "    rsum = 0\n",
    "\n",
    "    number_of_actions = utes #again from earlier in the code  \n",
    "    number_of_states = 120\n",
    "\n",
    "    start_state = int(wealth/10) #this is tied to wealth/ 10\n",
    "    state =start_state #try int wealth for states\n",
    "\n",
    "    gamma = 0.95\n",
    "    learning_rate = 0.10\n",
    "    egreedy = 0.1\n",
    "    \n",
    "    episodes = episodes #500k worked well? as did 1m\n",
    "    Mark = Market(kappa, episodes, time_periods, mu, rf, sigma) #parameters from earlier\n",
    "    \n",
    "    if model==1:\n",
    "        #Q learning no tabular model\n",
    "        SARSA = False\n",
    "        double = True #temp change all double\n",
    "        \n",
    "        agent = GeneralQ(number_of_states, number_of_actions, state, SARSA, \n",
    "                     double, step_size=learning_rate) #here eps set to 0.1 anyway\n",
    "        action = agent.behaviour_policy(agent._q[state])\n",
    "        \n",
    "    elif model==2:\n",
    "        #Double Q learning no tabular model\n",
    "        SARSA = False\n",
    "        double = True\n",
    "        \n",
    "        agent = GeneralQ(number_of_states, number_of_actions, state, SARSA, \n",
    "                     double, step_size=learning_rate) #here eps set to 0.1 anyway\n",
    "        action = agent.behaviour_policy(agent._q[state])\n",
    "        \n",
    "    elif model==3:\n",
    "        #Sarsa learning no tabular model\n",
    "        SARSA = True\n",
    "        double = True #temp change all double\n",
    "        \n",
    "        agent = GeneralQ(number_of_states, number_of_actions, state, SARSA, \n",
    "                     double, step_size=learning_rate) #here eps set to 0.1 anyway\n",
    "        action = agent.behaviour_policy(agent._q[state])\n",
    "        \n",
    "    elif model==4:\n",
    "        #Double Sarsa learning no tabular model\n",
    "        SARSA = True\n",
    "        double = True\n",
    "        \n",
    "        agent = GeneralQ(number_of_states, number_of_actions, state, SARSA, \n",
    "                     double, step_size=learning_rate) #here eps set to 0.1 anyway\n",
    "        action = agent.behaviour_policy(agent._q[state])\n",
    "    \n",
    "    elif model==5:\n",
    "        #Experience replay tabular Q learning - just 5 replays for now...as it doesn't learn\n",
    "        SARSA = False\n",
    "        double = True #change this temporarily and keep all double\n",
    "        \n",
    "        agent = ExpTabAgent(number_of_states, number_of_actions, state,\n",
    "                           SARSA, double, eps=egreedy, model=TabularModel, \n",
    "                            step_size=learning_rate, num_offline_updates=exprep)\n",
    "        action = agent.behaviour(state)\n",
    "    \n",
    "    elif model==6:\n",
    "        #Experience replay double tabular Q learning\n",
    "        SARSA = False\n",
    "        double = True\n",
    "        \n",
    "        agent = ExpTabAgent(number_of_states, number_of_actions, state,\n",
    "                           SARSA, double, eps=egreedy, model=TabularModel, \n",
    "                            step_size=learning_rate, num_offline_updates=exprep)\n",
    "        action = agent.behaviour(state)\n",
    "    \n",
    "    elif model==7:\n",
    "        #Dyna Q learning\n",
    "        SARSA = False\n",
    "        \n",
    "        agent = DynaQ(number_of_states, number_of_actions, state,\n",
    "           model=TabularModel, num_offline_updates=exprep, step_size=0.1, eps=egreedy)\n",
    "        \n",
    "        action = agent.behaviour_policy(agent._q[state])\n",
    "        \n",
    "    #action = agent.behaviour_policy(agent._q[state]) #if dyna\n",
    "    #action = agent.behaviour(state) #if exptab\n",
    "\n",
    "    for i_episode in range(episodes-1):\n",
    "        state = start_state\n",
    "\n",
    "        while True:    \n",
    "            prop = u_star[action]\n",
    "            #reward, d, new_state, dx, done = Mark.step((prop, wealth))\n",
    "\n",
    "            reward, _, _, dx, done = Mark.step((prop, wealth))\n",
    "            \n",
    "\n",
    "            wealth += dx\n",
    "            new_state = int(wealth/10)\n",
    "\n",
    "            action = agent.step(reward, gamma, new_state)\n",
    "\n",
    "            #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "            #Q[state, action] = (1 - learning_rate) * Q[state, action] \\\n",
    "            #    + learning_rate * (reward + gamma * torch.max(Q[new_state])) #this is a simple torch implementation\n",
    "            rsum += reward\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                state = start_state\n",
    "                agent._s = state\n",
    "\n",
    "                if model == 5 or model == 6 :\n",
    "                    agent._last_action = agent.behaviour(state)\n",
    "                else:\n",
    "                    action = agent.behaviour_policy(agent._q[state]) #if dyna\n",
    "                    \n",
    "                wealth_episodes.append(wealth)\n",
    "                utilities_mod.append(np.log(wealth))\n",
    "                rewards_mod.append(rsum)\n",
    "                rsum = 0\n",
    "                wealth = 100.0\n",
    "                break \n",
    "    \n",
    "    #store the q_values for later testing\n",
    "    q_name = 'LogUtestudt-doubles'+str(model)+'epi'+str(episodes)+'er'+str(exprep)+'kappa'+str(kappa*1000)\n",
    "    filename = 'q_tables/diffmodels/'+q_name\n",
    "    np.save(filename, agent.q_values)\n",
    "    \n",
    "    #print(agent.q_values)\n",
    "    print(q_name + \" last 50,000 rewards mean\",np.mean(np.array(rewards_mod)[-50000:]))\n",
    "    \n",
    "    return agent.q_values, utilities_mod, rewards_mod, wealth_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am thinking of the following 'story' - \\n1) Utility theory and key results \\n2) Stochastic processes and optimal multi-period investment \\n3) Merton's result \\n4) Ritter paper reformulating end utilities into incremental rewards \\n5) Reinforcement learning intro and toolbox \\n5) Implement Ritter paper to prove trading an OU process with and without costs \\n6) Show how Ritter can be transferred to Merton problem \\n7) Implementation - monte carlo sim,from analytic to numeric solution, mean var equivalent - \\nreformulating kappa, reformulating MV equivalent to a market environment for Q learning with incremental rewards \\n8) results with Q learning...how it breaks down with noise, what it learned...\\n9) methods to improve - to control bias, noise, learn with fewer examples?\\n\\n\\nNeed to clean and tidy this for different processes, parameters, rl algos\\nThe q table set up is very simple, extensions? continous state action space?\\nfunction approx?\\n\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remember to start logging and saving all the work and tidy thisstuff to make compelling...\n",
    "#plan is ...\n",
    "\n",
    "#\n",
    "\"\"\"\n",
    "\n",
    "I am thinking of the following 'story' - \n",
    "1) Utility theory and key results \n",
    "2) Stochastic processes and optimal multi-period investment \n",
    "3) Merton's result \n",
    "4) Ritter paper reformulating end utilities into incremental rewards \n",
    "5) Reinforcement learning intro and toolbox \n",
    "5) Implement Ritter paper to prove trading an OU process with and without costs \n",
    "6) Show how Ritter can be transferred to Merton problem \n",
    "7) Implementation - monte carlo sim,from analytic to numeric solution, mean var equivalent - \n",
    "reformulating kappa, reformulating MV equivalent to a market environment for Q learning with incremental rewards \n",
    "8) results with Q learning...how it breaks down with noise, what it learned...\n",
    "9) methods to improve - to control bias, noise, learn with fewer examples?\n",
    "\n",
    "\n",
    "Need to clean and tidy this for different processes, parameters, rl algos\n",
    "The q table set up is very simple, extensions? continous state action space?\n",
    "function approx?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.sim_prices import make_prices\n",
    "from lib.execute_strat import execute_strat\n",
    "from lib.graphs import make_baseline_graphs, make_agent_graphs, plot_sample_paths, plot_disc_utility, \\\n",
    "                        plot_mv_equiv, plot_const_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johngoodacre/anaconda/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:124: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogUtestudt-doubles7epi3000000er0kappa8.0 last 50,000 rewards mean 3.62525769828\n"
     ]
    }
   ],
   "source": [
    "#take a look at results v a random agent and merton optimal\n",
    "#kappa=0.008 #lets try risk neutral\n",
    "#for i in range(1,2):\n",
    "\n",
    "#for i in range(1,8):\n",
    "    #double q learning for a range of kappa's\n",
    "kappa = 0.008\n",
    "model = 7\n",
    "episodes = 3000000\n",
    "Q, utilities_mod, rewards_mod, wealth_episodes = train_agent(kappa,model, episodes, exprep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_sims =300000\n",
    "#S, B, utility, means, variances = make_prices(mu, sigma, rf, utes, time_periods, dt, X0, B0,S0, u_star, num_sims)\n",
    "\n",
    "#if we know the answer is log utility and \n",
    "#we know that we have gbm then we can simulate to\n",
    "#see if our best action is close to the optimal ratio\n",
    "#according to merton's formula\n",
    "\n",
    "#we need this to compare the merton strategy to our learned agent\n",
    "#best_action = np.argmax(utility)\n",
    "\n",
    "merton_ratio = (mu-rf)/sigma**2\n",
    "best_action = np.argmin(np.abs(u_star-merton_ratio))\n",
    "\n",
    "#get the random and optimal agent values\n",
    "#run the various baseline graphs pre agent\n",
    "utilities_test_rand, rewards_test_rand, step_rew_rand, wealth_test_rand = execute_strat(kappa,mu, \n",
    "                                                                      rf, sigma, utes,u_star,best_action, 'Random', \n",
    "                                                                      time_periods=time_periods, wealth=wealth)\n",
    "utilities_test_best, rewards_test_best, step_rew_best, wealth_test_best = execute_strat(kappa, mu, rf, sigma, \n",
    "                                                                      utes, u_star, best_action,'Merton', \n",
    "                                                                      time_periods=time_periods, wealth=wealth)\n",
    "\n",
    "#block_utilities_test_rand = np.mean(np.array(utilities_test_rand).reshape(1000,-1),0)\n",
    "#block_utilities_test_best = np.mean(np.array(utilities_test_best).reshape(1000,-1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q = np.load('q_tables/prior_runs/best_q_so_far.npy')\n",
    "\n",
    "#add in wealth episodes so we can look at return v volatility\n",
    "#can look at wealth distribution as well as sharpe ratio\n",
    "\n",
    "#Q = np.load('q_tables/model1epi1000000er0.npy')\n",
    "\n",
    "utilities_test, rewards_test, step_rew_test, wealth_test = execute_strat(kappa, mu, rf, sigma, utes, u_star,best_action, \n",
    "                                                            'Agent', q_values=Q, \n",
    "                                                            time_periods=time_periods, wealth=wealth)\n",
    "\n",
    "#block_utilities_test = np.mean(np.array(utilities_test).reshape(1000,-1),0)\n",
    "\n",
    "results = make_agent_graphs(rewards_test_best, rewards_test_rand, rewards_test,\n",
    "                  utilities_test_best, utilities_test_rand, utilities_test,\n",
    "                         wealth_test_rand, wealth_test_best, wealth_test)\n",
    "\n",
    "#make_agent_graphs(block_utilities_test_best,block_utilities_test,\n",
    "#                         block_utilities_test_rand, utilities_test_best,\n",
    "#                         utilities_test_rand, utilities_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
